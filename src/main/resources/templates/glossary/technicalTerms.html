<!DOCTYPE html>
<html lang="fr">

<head>
    <title>Le jargon technique dans Consol'Story</title>
    <!--    Tags meta   -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Retrouvez les termes spécifiques à l'informatique">
    <!--    Links   -->
    <link rel="stylesheet" type="text/css" media="screen" href="technicalTerms.css">
    <link rel="shortcut icon" href="../../static/logo/red_logo.png" type="image/x-icon">
</head>

<body>
<header>
    <nav>
        <ul>
            <li>@TODO : Faire une barre de navigation</li>
        </ul>
    </nav>
</header>

<main>
    <article>
        <h1>Le jargon technique expliqué</h1>

        <h2 id="api">API</h2>
        <p>
            Une interface de programmation d’application - souvent désignée par le terme API pour <em>Application Programming Interface</em> - est un ensemble normalisé
            de classes, de méthodes, de fonctions et de constantes qui sert de façade par laquelle un logiciel offre des services à d'autres logiciels. Elle est offerte
            par une bibliothèque logicielle ou un service web, le plus souvent accompagnée d'une description qui spécifie comment des programmes consommateurs peuvent se
            servir des fonctionnalités du programme fournisseur. De manière plus générale, on parle d'API à partir du moment où une entité informatique cherche à agir
            avec ou sur un système tiers, et que cette interaction se fait de manière normalisée en respectant les contraintes d'accès définies par le système tiers. On
            dit que le système tiers <em>expose une API</em>. À ce titre, des choses aussi diverses que la signature d'une fonction, une URL, un RPC… sont parfois
            considérés comme des API à part entière.
            <br>
            Dans l'industrie contemporaine du logiciel, les applications informatiques se servent de nombreuses interfaces de programmation, car la programmation se fait
            en réutilisant des briques de fonctionnalités fournies par des logiciels tiers. Cette construction par assemblage nécessite pour le programmeur de connaître
            la manière d’interagir avec les autres logiciels, qui dépend de leur interface de programmation. Le programmeur n'a pas besoin de connaître les détails de la
            logique interne du logiciel tiers, et celle-ci n'est pas nécessairement documentée par le fournisseur. Seule l'API est réellement nécessaire pour utiliser le
            système tiers en question. Des logiciels tels que les systèmes d'exploitation, les systèmes de gestion de base de données, les langages de programmation, ou
            les serveurs d'applications comportent une ou plusieurs interface(s) de programmation.
        </p>

        <h2 id="asg">Autostéréogramme</h2>
        <p>
            Un autostéréogramme est une image qui donne l'illusion d'une scène en trois dimensions à partir d'une image en deux dimensions. Afin de percevoir des formes
            en 3D grâce aux autostéréogrammes, le cerveau doit faire un effort oculaire de convergence et de mise au point dissociée de l'accommodation.
            <br>
            La vision stéréo est la superposition de deux images similaires non-identiques, avec pour résultat l'illusion de solidité et de profondeur. Dans le cerveau
            humain, la vision binoculaire résulte de mécanismes complexes qui forment une représentation tridimensionnelle de l'image en faisant coïncider chaque point -
            ou ensemble de points - vu par un œil avec le point correspondant - ou ensemble de points correspondants - vu par l'autre œil. En utilisant le phénomène de
            disparité binoculaire, le cerveau donne à chaque point un indice de profondeur. Lorsque le cerveau se confronte à un motif qui se répète comme un papier
            peint, il n'arrive pas à bien superposer la vision de l'œil gauche et celle de l'œil droit. Si on regarde un motif qui se répète horizontalement, mais en
            faisant converger les yeux en un point situé derrière le mur, le cerveau va superposer un motif vu par l'œil gauche, et un autre motif similaire vu par l'œil
            droit pour former une image virtuelle "derrière" le mur. La profondeur à laquelle l'image virtuelle est vue dépend de la distance entre les deux motifs
            observés. Les autostéréogrammes utilisent ce principe pour créer des images 3D : si les motifs sont proches les uns des autres, l'image virtuelle paraît
            proche, alors que si les motifs sont éloignés les uns des autres, l'image virtuelle paraît lointaine. Les images virtuelles sont perçues comme un
            <em>trou</em> dans l'image de base.
        </p>

        <h2 id="basic">BASIC</h2>
        <p>
            Le langage Basic est conçu en 1964 par John Kemeny et Thomas Kurtz au Dartmouth College pour permettre aux étudiants des filières non scientifiques d'utiliser
            des ordinateurs. Le langage est implémenté par une douzaine d'étudiants, parmi lesquels Mary Keller, une des premières personnes et la première femme à
            obtenir un doctorat en informatique aux Etats-Unis. À cette époque, les langages de programmation sont plus adaptés aux cartes perforées qu'aux terminaux
            interactifs. Même le Fortran II, peu évolué bien que déjà complexe, est inadapté aux calculs matriciels dont on fait déjà usage en sciences humaines. Non
            interactifs, ces langages exigent qu'un programme soit exempt de toute erreur de syntaxe pour être exécuté.
            <br>
            Le Basic se veut interactif et simple : ses sept instructions doivent pouvoir être enseignées en une demi-journée, et des opérations matricielles être
            exécutables en mode calcul de bureau. Un programme doit pouvoir s'exécuter au moins jusqu'à ce que soit rencontrée une première erreur, facilitant ainsi
            l'apprentissage du langage. Le destiner au domaine public favorise sa diffusion, et l'écriture d'un interpréteur de Basic fait partie des projets classiques
            donnés aux étudiants en informatique dans le monde entier.
            <br>
            Le Basic équipe dès le milieu des années 70 presque tous les micro-ordinateurs du moment - Olivetti P6060, Tektronix 4051, IBM 51001, Commodore PET... -. Dans
            les années 80, la plupart des micro-ordinateurs sont fournis avec un interprète Basic, parfois en ROM, et quelques calculatrices programmables en sont même
            dotées. Le premier IBM PC peut démarrer sans disquette et donne alors la main à un Basic rudimentaire en mémoire morte : la Cassette BASIC.
        </p>
        <h3>Son histoire...</h3>
        <p>
            Le Basic est peut-être le langage de programmation le plus connu. La forme originelle du langage s'inspire du Fortran, avec des ajouts pour le rendre
            interactif et capable de traiter en mode calcul notamment multiplication et inversion. Le Basic est souvent interprété, mais rien ne l'empêche d'être compilé
            là où l'interactivité avec le programmeur n'est plus nécessaire. Malgré sa normalisation, plusieurs dialectes sont apparus au cours des années, partis de la
            même base, et proposant des améliorations diverses, par exemple dans le domaine des interfaces graphiques ou de l'orienté objet. La référence sur PC - MS-DOS
            - est le QuickBasic.
            <br>
            La première version du Basic est développée sur un ordinateur appelé GE-265 de General Electric. Cet ordinateur 20 bits, conçu en 1959 au sein du département
            d'informatique industrielle par Arnold Spielberg, père de Steven Spielberg, occupe une pièce entière, embarquant 10 000 transistors et 20 000 diodes. Les
            données sont conservées sur des bandes magnétiques enroulées sur des bobines, des cartes perforées ou des bandes de papier. Vendu 210 000€ à l’époque, le
            GE-225 est un succès commercial.
            <br>
            Le Basic a en 1970 un concurrent, le langage FOCAL, mais celui-ci est propre à DEC - <em>Digital Equipment Corporation</em> - et se retrouve rapidement
            marginalisé, comme le sera le HPL de Hewlett-Packard six ans plus tard. Le Basic offre aussi quelques commandes comme old, new, list et quelques autres déjà
            définies à l'intérieur même du langage, lui conférant ses capacités interactives. Chaque instruction est précédée d'un nombre, en général attribué de 10 en
            10, qui permet de remplacer une instruction ou d'en intercaler d'autres sans avoir à maîtriser un complexe éditeur de texte en mode machine à écrire.
            <br>
            Un article devenu très populaire d'Edsger Dijkstra en 1968 insiste sur la nocivité de l'instruction <em>goto</em> en matière de qualité du code - déjà
            problématique à l'époque mais vraiment pris en considération que récemment du fait des attaques de hackers, obligeant les développeurs à être plus
            attentifs pour minimiser les failles des applications -, et donc à la productivité du programmeur. Cette critique s'applique alors à la plupart des langages,
            dont les dialectes du Basic. Dix ans plus tard, presque tous les langages prennent cette critique en compte. La popularité du Basic, langage de programmation
            grand public par excellence, fait dire à certains que ce langage a donné naissance au plus mauvais programme qu'aucun autre langage. De fait, ce langage aux
            contrôles sommaires ne convient qu'à la <em>petite programmation</em> de moins de 500 lignes.
            <br>
            Les versions les plus répandues sont les interpréteurs conçus par Microsoft qui s'impose comme la référence : quelles que soient les machines source et cible,
            un programme écrit en Basic Microsoft tourne sur un autre Basic Microsoft. Microsoft sort son premier logiciel, l'Altair Basic en 1977 - adaptation du
            Basic par Bill Gates et Paul Allen - pour l'Altair 8800 du constructeur MITS. Microsoft obtient d'IBM la commercialisation de son interprète Basic avec les
            futurs IBM PC et compatible PC en 1979. Cette version est incluse dans la puce ROM des PCs, et se lance au démarrage en l'absence de système d'exploitation.
            Plus tard, Microsoft vendra différentes versions du Basic pour DOS, dont Basica, GW-Basic, QuickBasic et Visual Basic pour MS-DOS. Windows 95 et Windows 98
            incluent un interpréteur QBasic à installer à partir du CD-ROM et Windows 98 inclut un interprète VBScript. <em>Visual Basic for Applications</em> est ajouté
            dans les produits Office en 1997.
            <br>
            La nécessité d'enseigner un langage de programmation davantage structuré donne naissance au langage Pascal en 1972. Ce langage, en particulier grâce au
            compilateur Turbo Pascal de la société Borland, bien plus rapide que n'importe quel interprète Basic de l'époque, remporte un énorme succès au cours des
            années 80, et marque un début de déclin de la popularité du Basic dans le grand public.
            <br>
            Les dialectes Basic modernes n'emploient plus nécessairement les numéros de lignes - qui restent possibles mais qui n'ont plus qu'un rôle éditorial - et ont
            une richesse de commandes et une construction des déclarations de données identiques à d'autres langages comme le langage Pascal. Les récentes variantes comme
            le Visual Basic introduisent une orientation objet avec gestion de l'implémentation d'interfaces et, dans Visual Basic .NET, l'héritage. Cependant la gestion
            des erreurs nécessite généralement l'utilisation de GOTO, ce qui casse la structuration des programmes. Visual Basic .NET permet l'utilisation de blocs
            Try/Catch - personnalisation d'une page d'erreur en lieu et place d'une page emplie de lignes de code -, mais conserve néanmoins la possibilité d'utiliser des
            <em>On Error GoTo</em>, pour conserver une certaine compatibilité avec des versions antérieures de Visual Basic. Le mouvement du Basic reste de loin le plus
            important et une société comme Niakwa vécut très largement de son Basic sur Unix jusqu'au milieu des années 90.
        </p>

        <h2 id="betamax">Betamax</h2>
        <p>
            Betamax est un format de cassette à ruban vidéo créé par Sony en 1975, il est destiné aux enregistrements d'émissions télévisées. D'autres fabricants
            produisent des magnétoscopes Betamax sous le nom Betacord : Toshiba, Pioneer, Aiwa, NEC, Zenith Electronics, Wega - Wuerttembergische Radio-Gesellschaft - et
            Sanyo. Son concurrent, le format VHS, s'imposera en dépit de performances qualitatives inférieures en 1985.
            <br>
            La raison du succès de la VHS sur le Betamax réside dans sa capacité d'enregistrement plus importante. En effet, la société RCA désirait une capacité
            d'enregistrement de 4 h mais Sony refusa - le Betamax a alors une capacité de 1 h - car ses ingénieurs estiment que la qualité vidéo eut été trop dégradée en
            accélérant la vitesse de défilement de la bande de 4 à 2 cm/s et en rétrécissant la piste vidéo - Betamax = Sony, VHS = Philips, la prochaine fois que vous
            souhaitez acheter un produit de qualité, réfléchissez-y ! -. Matsushita, malgré les protestations de JVC, fournit le mode LP - Long Play : durée
            d'enregistrement doublée par ralentissement du défilement de la bande - et accéda à la demande de RCA - Matsushita est réputée à cette époque de produire de
            la merde en même temps -. Le rapport prix/capacité d'enregistrement et le marketing des magnétoscopes VHS furent décisifs.
            <br>
            La qualité du Betamax est alors meilleure que celle de la VHS, mais l'introduction des vitesses B-II et B-III - capacités d'enregistrement de 2 et 3 h -
            réduisit la qualité vidéo du Betamax et descendit à celle de la VHS. Sony arrête la production des Betamax à partir de mars 2016, après 41 ans de production.
        </p>

        <h2 id="bluray">Blu-Ray</h2>
        <p>
            Le disque Blu-ray est un format de stockage de données de disque optique numérique conçu pour remplacer le format DVD, capable de stocker plusieurs heures de
            vidéo en haute définition - HDTV 720p et 1080p -. L'application principale du Blu-ray est pour le matériel vidéo tel que les longs métrages et pour la
            distribution physique de jeux vidéo pour PlayStation 3, PlayStation 4 et Xbox One. Le nom <em>Blu-ray</em> fait référence au laser bleu - qui est en fait
            violet - utilisé pour lire le disque, qui permet de stocker les informations à une densité plus élevée que ce qui est possible avec le laser rouge des DVD. Le
            disque en plastique mesure 120 millimètres de diamètre et 1,2 millimètres d'épaisseur, soit la même taille que les DVD et les CD. Les disques Blu-ray
            conventionnels contiennent 25 Go par couche, les disques double couche 50 Go, cela étant la norme de l'industrie pour les disques vidéo longs métrages. Des
            disques triple couche 100 Go et des disques quadruple couche 128 Go sont disponibles pour les lecteurs de réécriture BD-XL. La vidéo haute définition est
            stockée sur des disques Blu-ray avec une résolution maximale de 1920 × 1080 pixels, à 24 images/seconde. Sony dévoile les premiers prototypes de disques
            Blu-ray en octobre 2000, le premier prototype de lecteur sort au Japon en avril 2003 et en sortie mondiale le 20 juin 2006, débutant la guerre de la haute
            définition car le Blu-ray Disc se retrouve en concurrence avec le format HD DVD. <a href="companies.html#toshiba" title="Toshiba">Toshiba</a>, la principale
            société prenant en charge le HD DVD, s'avoue vaincu en février 2008 et sort son propre lecteur de disque Blu-ray fin 2009. 44% des ménages américains
            possédent un lecteur Blu-ray à janvier 2016. Pour la lecture de contenu 4K, le <em>Blu-Ray Disc Association</em> introduit une variante du Blu-ray appelée
            Ultra HD Blu-ray... jusqu'à l'arrivée imminente de la 8K.
        </p>

        <h2 id="ced">Capitance Electronic Disc</h2>
        <p>
            Le disque électronique capacitif - CED - est un système de lecture de disque vidéo analogique développé par RCA, dans lequel la vidéo et l'audio peuvent être
            lus sur un téléviseur à l'aide d'une aiguille spéciale et d'un système de rainure haute densité similaire aux enregistrements phonographiques. Conçu pour la
            première fois en 1964, le système CED est considéré comme un succès technologique capable d'augmenter la densité d'un disque. Malgré cette réussite, le CED
            est victime d'une mauvaise planification, de divers conflits avec la direction, de plusieurs difficultés techniques qui ralentissent le développement et
            bloquent la production du système pendant 17 ans - jusqu'en 1981, date à laquelle il est rendu obsolète par le laser vidéodisques - DiscoVision plus tard
            appelé LaserVision et LaserDisc - ainsi que les formats de cassette vidéo Betamax et VHS. RCA arrête la production de lecteurs au printemps 1984 mais continue
            la production de vidéodisques jusqu'en 86, perdant ainsi environ 510 millions d'€. Le format connu sous le nom de <em>vidéodisque</em> porte d'ailleurs à
            confusion avec le format LaserDisc contemporain. Les LaserDiscs sont lus avec un faisceau laser, tandis que les disques CED sont lus physiquement avec un
            stylet - similaire à un enregistrement gramophone conventionnel -. RCA utilise la marque <em>SelectaVision</em> pour le système CED, également utilisé pour
            certains de ces premiers magnétoscopes.
        </p>

        <h2 id="sd">Carte SD</h2>
        <p>
            Une carte SD - Secure Digital - est une carte mémoire amovible de stockage de données numériques créée en janvier 2000 par une alliance formée entre les
            industriels Panasonic, SanDisk et Toshiba. Les cartes SD sont utilisées pour le stockage de fichiers dans les appareils photo numériques, les caméscopes
            numériques, les systèmes de navigation GPS, les consoles de jeux vidéo, les téléphones mobiles ou encore les systèmes embarqués. Depuis 2010, les cartes SD
            font office de standard de stockage, à la suite de l'abandon progressif des autres formats, notamment du Memory Stick de Sony. Au sein des cartes SD, on
            distingue plusieurs normes selon les capacités de stockage des cartes : SD - jusqu'à 2 Go -, SDHC - de 2 à 32 Go -, SDXC - de 32 Go à 2 To - et SDUC - de 2 à
            128 To -. Le terme carte SD regroupe 3 différents types de produits : les carte SD, carte miniSD, carte microSD.
        </p>
        <h3>Les cartes SD</h3>
        <p>
            SanDisk, Matsushita et Toshiba choisissent de développer et de commercialiser la carte mémoire Secure Digital - SD - en 1999, dérivée de la carte
            MultiMediaCard - MMC -, qui assure la gestion des droits numériques selon la norme <em>Secure Digital Music Initiative</em> - SDMI -. Elle est conçue pour
            concurrencer la Memory Stick, un produit lancé par Sony l'année précédente. Le logo SD est créé à l'origine pour le disque <em>Super Density Disc</em>, qui
            échoua dans la guerre du format DVD. C'est pourquoi le 'D' du logo ressemble à un disque optique. Lors du
            <a href="https://www.youtube.com/watch?v=1JVr3hQUlXg" title="Consumer Electronics Show 2000">Consumer Electronics Show 2000</a>, les trois sociétés annoncent
            la création de la SD Association afin de promouvoir les cartes SD. La SD Association, dont le siège est situé à San Ramon en Californie, compte une trentaine
            de sociétés à sa création, et compte aujourd'hui environ 1 000 fabricants de produits qui produisent des cartes et des périphériques interopérables. Les
            premiers échantillons de la carte SD sont disponibles au premier trimestre 2000, pour voir apparaître, trois mois plus tard, les premières cartes de 32 et 64
            Mo sur le marché.
        </p>
        <h3>Les cartes mini-SD</h3>
        <p>
            Le format mini-SD est créé en 2003 par la SD Association. Il dérive du format SD classique mais est physiquement plus petit. Contrairement au format SD, prévu
            pour une utilisation dans les appareils photo numériques, le format miniSD est utilisé essentiellement dans la téléphonie mobile. Les cartes miniSD sont
            totalement compatibles avec les cartes SD, à travers un adaptateur mécanique, généralement fourni avec les cartes. Le format miniSDHC existe aussi, dérivé des
            cartes SDHC et offre les mêmes avantages que ce dernier.
        </p>
        <h3>Les cartes micro-SD</h3>
        <p>
            Le format micro-SD est créé en 2005 toujours par la SD Association. Il s'agit d'une adaptation de la norme TransFlash, format de cartes mémoire créé par
            SanDisk. Plus petit que le format SD classique, il vise un usage dans les appareils photo numériques et un remplacement éventuel des clés USB. Le format
            microSD cible les téléphones mobiles et les tablettes, et majoritairement utilisé en 2012 par les constructeurs de téléphones. Il est rapidement remplacé le
            format miniSD car plus petit tout en offrant les mêmes avantages. Les cartes microSD sont totalement compatibles avec les cartes SD et les cartes miniSD, à
            travers un adaptateur mécanique, souvent fourni avec les cartes.
        </p>

        <h2 id="hucard">Cartouche HuCard</h2>
        <p>
            HuCard est un format de carte mémoire développé par Hudson Soft à la fin des années 80 et utilisé comme support pour les jeux vidéo des consoles de la gamme
            PC-Engine. Le circuit intégré miniaturisé permet des dimensions très proches d'une carte de crédit et nettement plus compactes que les formats de cartouches
            utilisés chez les marques concurrentes à la même époque. Avec une face seulement protégée par un film plastique noir, le circuit dispose de 38 connecteurs. Il
            s'agit typiquement d'une ROM à capacité de stockage variable et contenant le code d'un jeu, mais il peut aussi contenir de la mémoire réinscriptible pour la
            sauvegarde des parties, ou même de la RAM pour faire évoluer les capacités techniques des consoles. La commercialisation des HuCards débute en 1987 au Japon
            avec les jeux destinés au premier modèle PC-Engine. Afin d'assurer la rétro-compatibilité, il est conservé sur tous les modèles qui suivirent : CoreGrafX,
            SuperGrafX, Duo... jusqu'en 1999 avec le dernier jeu officiellement édité, soit cinq ans après la sortie de la dernière console de la gamme. Par ailleurs, la
            HuCard est zonée pour le marché nord-américain et rebaptisée TurboChip en référence à la TurboGrafx-16, nom donné à la déclinaison américaine de la PC-Engine.
        </p>

        <h2 id="cdrom">CD-ROM</h2>
        <p>
            Un CD-ROM - <em>Compact Disc Read Only Memory</em> - Disque compact à mémoire de lecture seule - est un disque optique utilisé pour stocker des données
            sous forme numérique destinées à être lues par un ordinateur ou tout autre lecteur compatible - salon, console de jeu... -. Le CD-ROM est une évolution du CD
            audio, destiné uniquement aux données numériques musicales prévues pour un lecteur CD de chaîne HI-FI ou de baladeur. Grâce à leur grande capacité de stockage
            et leur compacité, les cédéroms supplantent les disquettes dans la distribution des logiciels et autres données informatiques.
            <br>
            Le CD-ROM est inventé par Philips en 1979 et commercialisé en 1982 par Philips et Sony. Un cédérom contient des données non modifiables ou modifiables selon
            que le disque est réinscriptible ou non, écrit grâce à un graveur et lu par un lecteur CD. C'est un disque optique en matière plastique très léger, de 12 cm
            de diamètre, pouvant contenir jusqu'à 700 Mo de données informatiques, soit 74 ou 80 minutes d’enregistrement audio dans le format original.
            <br>
            Comme tout support d’informations numériques, le CD-ROM permet une bonne conservation théorique des données. C'est un disque lu sans contact par le laser du
            lecteur et n’est pas soumis à une usure mécanique directe. Comme disque optique, les données contenues ne peuvent pas être affectées par un champ magnétique.
            Cependant, le CD-ROM enregistrable - CD-RW - s'avère modérément fiable. Il est censé conserver les données durant une centaine d’années, mais ce serait plutôt
            dix voire cinq ans. Son matériau polymère est en effet sensible aux rayonnements ultraviolets émis par la lumière, à la chaleur, à l'humidité et aux rayures
            de surface qui entraînent rapidement des erreurs de lecture, puis une impossibilité totale de lecture.
        </p>

        <h2 id="chipset">Chipset</h2>
        <p>
            Un chipset - signifiant littéralement jeu de puces - est un jeu de composants électroniques inclus dans un circuit intégré préprogrammé, permettant de
            gérer les flux de données numériques entre le ou les processeur(s), la mémoire et les périphériques. On en trouve dans des appareils électroniques de type
            micro-ordinateur, console de jeux vidéo, téléphone mobile, appareil photographique numérique, GPS...
        </p>

        <h2 id="circuit">Circuits imprimés</h2>
        <p>
            Un circuit imprimé - <em>printed circuit board</em> - est un support, en général une plaque, permettant de maintenir et de relier électriquement un ensemble
            de composants électroniques entre eux, dans le but de réaliser un circuit électronique complexe. On le désigne aussi par le terme de carte électronique.
            <br>
            Constitué d'un assemblage d'une ou plusieurs fines couches de cuivre séparées par un matériau isolant, ces premières sont gravées par un procédé chimique pour
            obtenir un ensemble de pistes, terminées par des pastilles. Le circuit imprimé est souvent recouvert d'une couche de vernis coloré qui protège les pistes de
            l'oxydation et d'éventuels courts-circuits... Si le coeur vous en dit, ce
            <a href="https://fr.wikipedia.org/wiki/Circuit_imprim%C3%A9" title="en savoir plus">chemin</a> vous amènera à la connaissance ultime...
        </p>

        <h2 id="din">Connecteur DIN</h2>
        <p>
            Le connecteur DIN est normalisé par le <em>Deutsches Institut für Normung</em> - DIN -, l'agence allemande de normalisation. Dans le contexte de
            l'électronique grand public, le terme connecteur DIN désigne un connecteur circulaire utilisé pour la transmission des signaux audio. Certains de ces
            connecteurs sont utilisés par la suite pour la vidéo analogique et pour des interfaces numériques comme MIDI, ou la connexion des claviers et souris - port
            PS/2 -.
        </p>

        <h2 id="dediee">Console dédiée</h2>
        <p>
            Une console dédiée est une console de jeux vidéo conçue uniquement pour faire fonctionner un ou plusieurs jeux intégrés. À ce titre, une console dédiée ne
            dispose pas de port permettant l'insertion d'une cartouche, d'un disque optique ou de tout autre support. La console portable Game and Watch ou la Mini NES
            sont toutes deux des consoles dédiées. Le principe précède celui des consoles multi-jeux. En effet, les toutes premières consoles sont dédiées et ne peuvent
            faire fonctionner qu'un petit nombre de jeux, si ce n'est un jeu unique. C'est le cas de Pong et de ses imitations. Les années 70 voient la multiplication des
            consoles fonctionnant avec des cartouches de jeu, la première d'entre elles étant la Fairchild Channel F. Par évolution, les consoles majeures de l'histoire
            du jeu vidéo incarnent au fur et à mesure le modèle opposé. Historiquement, les consoles Nintendo jusqu'à la GameCube et Sega jusqu'à la Saturn acceptent les
            cartouches. Le disque optique se popularise vers la seconde moitié des années 90 avec la PlayStation et s'impose comme le support de stockage majoritaire de
            la cinquième génération de consoles et des générations suivantes jusqu'à l'avènement du DVD puis du Blu-Ray et vers la prochaine évolution de
            dématérialisation physique des jeux.
        </p>

        <h2 id="cpu">CPU</h2>
        <p>
            Une unité centrale de traitement - <em>Central Processor Unit</em> ou processeur central - est le circuit électronique d'un ordinateur qui exécute des
            instructions composant un programme informatique. L'UC exécute des opérations arithmétiques, logiques, de contrôle et d'entrée/sortie - In/Out - de base
            spécifiées par les instructions du programme. L'industrie informatique utilise ce terme depuis 1955. CPU fait référence à un processeur, plus spécifiquement
            à son unité de traitement et à son unité de commande - Command Unit -, distinguant ces éléments de base d'un ordinateur des composants externes tels que la
            mémoire principale et les circuits d'I/O. La forme, la conception et la mise en œuvre des processeurs changent au cours de leur histoire, mais leur
            fonctionnement fondamental reste pratiquement inchangé. Les principaux composants d'un processeur comprennent l'unité arithmétique et logique - Arithmetical
            Logical Unit - qui effectue les opérations arithmétiques et logiques, les registres de processeur qui fournissent des opérandes à l'ALU et stockent les
            résultats de ces opérations, et une unité de contrôle qui orchestre l'extraction à partir de la mémoire et l'exécution des instructions en dirigeant les
            opérations coordonnées de l'ALU, des registres et d'autres composants. La plupart des processeurs modernes sont des microprocesseurs, où le processeur est
            contenu sur une seule puce de circuit intégré - Integrated Circuit - en oxyde de métal semi-conducteur - Metal Oxyd Semi -. Un IC qui contient un processeur
            peut contenir de la mémoire, des interfaces périphériques et d'autres composants d'un ordinateur : ces dispositifs intégrés s'appellent microcontrôleurs ou
            systèmes sur puce - System On a Chip -. Certains ordinateurs utilisent un processeur multicœur, qui est une seule puce ou <em>socket</em> contenant au moins
            deux processeurs appelés <em>cœurs</em>. Les processeurs matriciels ou les processeurs vectoriels ont plusieurs processeurs qui fonctionnent en parallèle,
            aucune unité n'étant considérée comme centrale.
        </p>

        <h2 id="directx">DirectX</h2>
        <p>
            Microsoft DirectX est une collection de bibliothèques destinées à la programmation d’applications multimédia, plus particulièrement de jeux ou de programmes
            faisant intervenir de la vidéo sur les plates-formes Microsoft - Xbox, systèmes d’exploitation Windows -. À l’origine le nom de chacune de ces bibliothèques
            commençait par Direct : Direct3D, DirectDraw, DirectMusic, DirectPlay, DirectSound... DirectX étant le nom générique faisant référence à l’ensemble de ces
            technologies. Avec le temps cette convention de nommage est un peu tombée en désuétude, le X prenant l’ascendant des technologies intitulées Xact, Xinput et
            qui ont rejoint la grande famille des technologies DirectX. Ainsi lorsque Microsoft s’est lancé dans le développement d’une console faisant un usage intensif
            de ces technologies, le X est prédominant ce qui conduit au nom Xbox et, par la suite, Xbox 360 et Xbox One.
            <br>
            Direct3D - bibliothèque permettant de faire de la 3D temps réel - est largement utilisée dans le développement de jeux pour la plate-forme Microsoft Windows,
            pour Xbox et Xbox 360. Direct3D est aussi utilisé pour d’autres type d’applications s’appuyant sur des graphismes en 3D de haute qualité pour visualiser des
            données complexes bien que son concurrent OpenGL soit dans ce domaine mieux implanté car existante sur bien plus de platesformes. Direct3D étant peut-être une
            des technologies DirectX les plus reconnues, il n’est pas rare de voir le nom DirectX utilisé en lieu et place de Direct3D.
            <br>
            La multitude des composants DirectX existe sous deux formes. L’une uniquement destinée à faire tourner les applications développées en utilisant ces
            technologies - runtime components -, l’autre étant un kit de développement à l’usage des développeurs. Les versions de Direct3D 9Ex et 10 sont utilisés par
            Windows Vista. Ces deux versions font usage de fonctionnalités propres au nouveau modèle de pilote <em>Windows Display Driver Model</em> apparu avec Windows
            Vista. La nouvelle infrastructure graphique de Windows supporte la virtualisation du matériel graphique vis-à-vis de multiples applications et services comme
            le <em>Desktop Window Manager</em> - le gestionnaire de desktop de Vista lorsque Aero est activé -. Avec Windows XP, les applications ont un accès exclusif à
            la carte graphique et peuvent accaparer les ressources de celle-ci. Sur PC, DirectX est actuellement en version 12 sur Windows 10, version 11 pour Windows 7,
            Windows Vista et Windows Server 2008 et 9.0c pour toutes les versions antérieures de Windows.
        </p>

        <h2 id="haptique">Dispositif haptique</h2>
        <p>
            Un dispositif haptique est un système tactilo-kinesthésique physique ou mécanique, éventuellement robotique qui peut notamment créer une communication entre
            un humain et une partie de son environnement, le cas échéant entre un opérateur et un environnement virtuel. Il permet aux utilisateurs de concevoir, modeler
            et manipuler des objets dans un environnement virtuel avec un certain ressenti tactile - le toucher - et la perception kinesthésique - le retour de force,
            qui sont de plus en plus souvent utilisés comme retours sensoriels dans les systèmes de Réalité Virtuelle -. Les dispositifs haptiques sont développés par
            plusieurs sociétés en collaboration avec des centres de recherche - LVMH, PSA, EADS… -.
            <br>
            Le dispositif haptique se décompose en trois partie :
        </p>
            <ul>
                <li>
                    Stabilité : la capacité du dispositif haptique à imiter un mur virtuel. Le dispositif doit avoir la capacité de reproduire une raideur de contact lors
                    d'un contact avec un obstacle afin que l'utilisateur ne puisse pas avoir la possibilité de le traverser.
                </li>
                <li>
                    Transparence : l’utilisateur doit avoir l'impression de manipuler directement l’objet virtuel sans ressentir la dynamique de l’interface haptique.
                    Idéalement, le poids, l’inertie, les frottements du dispositif haptique ne doivent pas être perçus par l’utilisateur.
                </li>
                <li>
                    Résolution de position : la plus petite quantité de mouvement sur lequel les capteurs peuvent détecter un changement de position. Une bonne résolution
                    de la position est un facteur important de la démonstration de la rigidité des murs virtuels.
                </li>
            </ul>

        <h2 id="atmos">Dolby Atmos</h2>
        <p>
            Le Dolby Atmos est une technologie de son surround développée par Dolby Laboratories. Il étend les systèmes de son surround existants en ajoutant des canaux
            de hauteur, permettant aux sons d'être interprétés comme des objets tridimensionnels. Suite à la sortie d'Atmos pour le marché du cinéma, une variété de
            technologies grand public sont lancées sous la marque Atmos, utilisant des haut-parleurs encastrés et montés.
            <br>
            La première installation Dolby Atmos se fait au Dolby Theater de Los Angeles, pour la première de 'Brave' en juin 2012 et, sur le reste de l'année, sur 25
            installations dans le monde pour arriver à 4 400 emplacements en avril 2019. Dolby Atmos est adapté à un format de cinéma maison et est la composante audio
            de Dolby Cinema. La plupart des appareils électroniques depuis 2016, ainsi que les smartphones après 2017, sont activés pour l'enregistrement et le mixage
            Dolby Atmos. Game of Thrones est la première série télévisée mixée avec Atmos, en commençant par sa réédition Blu-ray 2016. L'album "Automatic for the People"
            de R.E.M. datant de 1992 est la première sortie musicale majeure avec sa réédition du 25e anniversaire en 2017.
            <br>
            La technologie Dolby Atmos permet de distribuer jusqu'à 128 pistes audio plus les métadonnées de description audio spatiale associées aux cinémas pour un
            rendu dynamique optimal vers les haut-parleurs en fonction des capacités du théâtre. Chaque piste audio est affectée à un canal audio, le format traditionnel
            de distribution. Dolby Atmos par défaut lit entre 7 et 10 canaux pour l'ambiance ou le dialogue central, laissant 118 pistes pour le reste. Le Home Cinéma
            Dolby Atmos est construit sur une disposition traditionnelle 5.1 et 7.1. Pendant la lecture, le système Dolby Atmos de chaque cinéma restitue les objets audio
            en temps réel de sorte que chaque son provient de son emplacement désigné par rapport aux haut-parleurs présents dans le théâtre contrairement à la
            technologie multicanal traditionnelle qui balance toutes les pistes audio source dans un nombre fixe de canaux pendant la post-production. L'ajout d'objets
            audio permet au mixeur d'être plus créatif, de faire sortir plus de sons de l'écran et d'être sûr des résultats. Le matériel de cinéma de première génération,
            le <em>Dolby Atmos Cinema Processor</em>, prend en charge jusqu'à 128 pistes audio distinctes et jusqu'à 64 flux de haut-parleurs uniques. En plus de
            reproduire un mixage 5.1 ou 7.1 standard à l'aide d'enceintes regroupées en tableaux, le système Dolby Atmos peut également donner à chaque enceinte sa propre
            alimentation unique en fonction de son emplacement exact, permettant ainsi de nombreuses nouvelles hauteurs en avant, en surround et même au plafond
            permettant aux canaux un panoramique précis de certains sons tels qu'un hélicoptère ou la pluie.
        </p>

        <h2 id="dram">DRAM</h2>
        <p>
            La mémoire vive dynamique - DRAM pour <em>Dynamic Random Access Memory</em> - est un type de mémoire vive compacte et peu dispendieuse. La simplicité
            structurelle de la DRAM — un pico-condensateur et un transistor pour un bit — permet d'obtenir une densité élevée. Son inconvénient réside dans les courants
            de fuite des pico-condensateurs : l'information disparaît à moins que la charge des condensateurs ne soit rafraîchie avec une période de quelques
            millisecondes. D'où le terme de dynamique. A contrario, les mémoires statiques SRAM n'ont pas besoin de rafraîchissement mais utilisent plus d'espace. Sans
            alimentation, la DRAM perd ses données, ce qui la range dans la famille des mémoires volatiles. Les puces mémoires sont regroupées sur des supports SIMM  -
            contacts électriques identiques sur les 2 faces du connecteur de la carte de barrette - ou DIMM - contacts électriques séparés sur les 2 faces du connecteur -.
        </p>

        <h2 id="dsp">DSP</h2>
        <p>
            Un DSP - <em>Digital Signal Processor</em>, Processeur de signal numérique - est un microprocesseur optimisé pour exécuter des applications de traitement
            numérique du signal - filtrage, extraction de signaux... - le plus rapidement possible. On les trouve dans les modems, les téléphones mobiles, les appareils
            multimédia - lecteur MP3 -, les récepteurs GPS... Ils sont également utilisés dans des systèmes vidéo, les chaînes de traitement de son, partout où l'on
            reçoit un signal complexe que l'on doit modifier à l'aide du filtrage.
        </p>

        <h2 id="dualshock">DualShock</h2>
        <p>
            La manette DualShock est épelée à l'origine en deux mots - Dual Shock - puis déposé sous les deux formes - DUALSHOCK ou DUAL SHOCK - tout comme le terme
            'DualSense' déposé pour la nouvelle gamme de manettes Sony. La DualShock est une manette de jeu avec retour de vibrations et commandes analogiques développées
            par Sony pour sa famille de systèmes PlayStation. La toute première DualShock est lancée au Japon en novembre 1997. D'abord introduit comme périphérique
            secondaire pour la PlayStation originale, une mise à jour de la console l'ajouta en contrôleur principal en lieu et place de la Dual-Analog.
        </p>
        <h3>Dual Analog</h3>
        <figure>
            <a href="../../static/img/sony/dual_analog.jpg"><img src="../../static/img/sony/dual_analog_mini.png" alt="DualAnalog"
                                                                 title="La Dual Analog, ancêtre de la DualShock"></a>
            <a href="../../static/img/sony/dual_sony.jpg"><img src="../../static/img/sony/dual_sony_mini.png" alt="DualSony" title="La DualShock et la Dual Analog"></a>
            <a href="../../static/img/sony/dual_buttons.jpg"><img src="../../static/img/sony/dual_buttons_mini.png" alt="DualButtons"
                                                                     title="La DualShock et la Dual Analog"></a>
        </figure>
        <p>
            Très semblable à la DualShock, la Dual Analog est la toute première manette PlayStation sortie en avril 1997. Contrairement à la future DualShock, cette
            manette possède un seul moteur de vibration pour la version japonaise. En effet, aucun jeu prévu en Europe et aux Etats-Unis ne nécessitait des effets de
            vibration, Sony décide donc de ne pas intégrer le moteur vibratoire pour des raisons de coûts. Restée seulement une année en vente, cette manette prépare la
            DualShock, tout d’abord au niveau des sticks. Ceux-ci sont concaves - creux - et le revêtement est lisse, loin du rugueux arrondis de la DualShock. Ces sticks
            sont très sensibles et il faut les bouger à minima pour éviter de partir dans tous les sens - ce qui peut-être très énervant surtout avec un jeu à réaction
            rapide nécessitant de plus une bonne précision du style FPS. La prise en main est bien meilleure que sur les autres manette PS1 grâce aux poignées plus
            longues - une taille d'ailleurs reprise pour la nouvelle DualSense. Une autre caractéristique sur cette Dual Analog est la longueur des boutons L2 et R2.
            Contrairement à la manette PS1 classique, les boutons L2 et R2 sont de tailles différentes ce qui permet de mieux les identifier. Une bonne idée de cette
            Dual Analog abandonnée par la suite : les boutons L2 et R2 sont plus éloignés des L1 et R1 et ont un rebord bienvenu. Le bouton Analog possède 3 modes, le
            premier est le mode digital avec les sticks inactifs et donc la LED est éteinte. Le second mode Analog avec la LED rouge exactement comme la DualShock et
            enfin le mode Flightstick avec la LED verte. Le bouton ressort d’ailleurs plus que sur la DualShock et les joueurs ont eu tendance à appuyer dessus par
            inadvertance, Sony a du revoir sa copie par la suite. Pour information, le mode Analog ne peut pas être verrouillé ou modifié par défaut par un jeu comme
            c’est le cas après avec la DualShock.
        </p>
        <h3>DualShock</h3>
        <figure>
            <a href="../../static/img/sony/dualShock.jpg"><img src="../../static/img/sony/dualShock_mini.png" alt="DualShock"
                                                              title="La DualShock, première de sa lignée"></a>
        </figure>
        <p>
            Le contrôleur analogique DualShock - SCPH-1200 - est basé sur les actions à l'écran se déroulant dans le jeu, ainsi que sur l'entrée analogique via deux
            sticks analogiques. Son nom dérive de son utilisation de doubles moteurs de vibration. Ces moteurs sont logés dans les poignées du contrôleur, celui de gauche
            étant plus gros et plus puissant que celui de droite, de manière à permettre des niveaux de vibration variables. Le DualShock diffère de la Rumble Pak de la
            Nintendo 64 à cet égard car la Rumble Pak n'utilise qu'un seul moteur et utilise l'énergie de la batterie pour la fonction de vibration, mais toutes les
            variétés filaires de la DualShock utilisent l'alimentation fournie par la PlayStation. La fonction de grondement de la DualShock est similaire à celle de la
            première édition du contrôleur japonais Dual Analog, une fonctionnalité qui est supprimée peu de temps après la sortie de cette manette. La DualShock a deux
            sticks analogiques comportant des poignées en caoutchouc texturé plutôt que les pointes en plastique lisses avec des rainures en retrait trouvées sur le
            contrôleur Dual Analog. Les autres différences entre Dual Analog et DualShock incluent les poignées plus longues et les boutons L2 / R2 légèrement différents.
            Le contrôleur Dual Analog dispose également d'un mode supplémentaire accessible en appuyant sur le bouton <em>Analog</em> qui assure la compatibilité avec le
            joystick analogique PlayStation, ce qui fait que le voyant analogique devient vert au lieu de rouge - le mode Flightstick -. Peu de temps après son lancement,
            la plupart des nouveaux jeux incluent la prise en charge de la fonction de vibration et de doubles sticks analogiques. De nombreux jeux profitent de la
            présence de deux moteurs pour fournir des effets de vibration en stéréo notamment Gran Turismo et Quake II. Sorti en 1999, le hit PlayStation
            <a href="https://www.youtube.com/watch?v=KZY8F0SiSFY" title="Ape Escape">Ape Escape</a> est le premier jeu à exiger explicitement des contrôleurs de type
            DualShock / Dual-Analog, son gameplay nécessitant l'utilisation des deux sticks analogiques. La PSOne sort avec le contrôleur DualShock en 2000 légèrement
            redessiné, similaire au premier, sa couleur étant blanche au lieu de gris, le logo est <em>PSOne</em> au lieu de <em>PlayStation</em>, la plupart des boutons,
            les sticks analogiques et le cordon sont plus brillants que le précédent, et le connecteur est plus en forme de demi-cercle que de bord arrondi. Le DualShock
            est compatible avec la PlayStation 2, car ils utilisent le même connecteur et le même protocole sauf certaines exceptions comme
            <a href="https://www.youtube.com/watch?v=TnHTQz_FMhY&t=492s" title="The Bouncer">The Bouncer</a>.
        </p>
        <h3>DualShock 2</h3>
        <figure>
            <a href="../../static/img/sony/dualShock2.jpg"><img src="../../static/img/sony/dualShock2_mini.png" alt="DualShock 2" title="La DualShock 2 pour PS2"></a>
        </figure>
        <p>
            Le contrôleur analogique DualShock 2 - SCPH-10010 - est inclus avec le lancement de PlayStation 2 et presque identique à l'extérieur de la DualShock
            précédente, avec quelques changements cosmétiques mineurs. Un logo bleu DualShock 2 est ajouté sur le dessus du contrôleur, le connecteur est plus carré que
            la DualShock, le câble, le connecteur et la manette sont noirs au lieu du gris. Les sticks analogiques sont également nettement plus rigides que sur la
            DualShock d'origine. A l'intérieur de la manette, la DualShock 2 est plus légère et tous les boutons - à l'exception du mode analogique, des boutons de
            démarrage, de sélection, L3 et R3 - sont sensibles à la pression exercée. La DualShock 2 est disponible en différentes couleurs: noir, argent satiné, blanc
            céramique, gris ardoise, bleu océan, vert émeraude, rouge cramoisi et rose bonbon. La PlayStation originale est compatible avec le DualShock 2. La PlayStation
            3 est rétrocompatible avec la DualShock et la DualShock 2 grâce à l'utilisation de périphériques tiers, qui connectent le contrôleur à la console via un port
            USB. Cependant, la DualShock et DualShock 2 ne fonctionnent pas correctement avec les jeux nécessitant la fonctionnalité Sixaxis, tels que
            <a href="https://www.youtube.com/watch?v=i63xO2FH-2I" title="Heavy Rain">Heavy Rain</a>.
        </p>
        <h3>DualShock 3</h3>
        <figure>
            <a href="../../static/img/sony/dualShock3.jpg"><img src="../../static/img/sony/dualShock3_mini.png" alt="DualShock 3" title="La DualShock 3 pour PS3"></a>
        </figure>
        <p>
            La manette sans fil DualShock 3 - SCPH-98050 - est une manette de jeu pour la PlayStation 3. Elle remplace la manette sans fil Sixaxis sortie avec les
            versions antérieures de la console. La DualShock 3 ajoute les capacités de retour haptique - le retour de force - aux capacités de la DualShock et de la
            DualShock 2. La fonction de vibration et les capacités de détection de mouvement de la DualShock 3 peuvent être utilisées simultanément sans que l'une
            interfère avec l'autre. La manette dispose d'un port USB mini-B pour le chargement et peut être utilisé sur une PSP via Bluetooth. La DualShock 3 peut être
            identifié par ses inscriptions <em>DualShock 3</em> et <em>Sixaxis</em>. Elle pèse 192 grammes, soit 40% de plus que son prédécesseur qui ne pèse que 137
            grammes. Les marques arrière indiquent la consommation électrique pouvant monter jusqu'à 300 mA de courant, ce courant n'est pas tiré en permanence et est
            maximum lorsque le grondement est actif. Sa principale source d'alimentation est une batterie Li-ion interne de 3,7 V capable de stocker 570 mA, ce qui offre
            jusqu'à 30 heures de jeu continu sur une charge complète. La DualShock 3 peut être alimenté via un connecteur USB mini-B sur le dessus du contrôleur. Cela
            permet au contrôleur d'être utilisé lorsque la batterie est faible tout en chargeant la batterie. Parallèlement à la sortie du modèle PS3 slim, Sony sort une
            nouvelle version de la DualShock 3. Cette révision supprime les supports en plastique supplémentaires entre les boutons L1 / R1 et les déclencheurs L2 / R2 -
            augmentant la rigidité du contrôleur -, a des voyants soudés directement sur la carte et est livré dans des schémas de couleurs légèrement révisés.
        </p>
        <h3>DualShock 4</h3>
        <figure>
            <a href="../../static/img/sony/dualShock4.jpg"><img src="../../static/img/sony/dualShock4_mini.png" alt="DualShock 4" title="La DualShock 4 pour PS4"></a>
        </figure>
        <p>
            La DualShock 4 - CUH-ZCT1 - est le contrôleur de la PlayStation 4. Elle possède plusieurs nouvelles fonctionnalités par rapport à la DualShock 3. Un pavé
            tactile capacitif à deux points intégré à l'avant du contrôleur qui peut être cliqué permettant de représenter plusieurs boutons, comme démontré dans la
            version PS4 d'<a href="https://www.youtube.com/watch?v=KZY8F0SiSFY" title="Elite Dangerous">Elite Dangerous</a> dans laquelle les quatre coins du pavé tactile
            peuvent être mappés à des actions cliquables distinctes. Le contrôleur prend en charge la détection de mouvement via un gyroscope, un accéléromètre et des
            vibrations. La manette comprend une batterie lithium-ion rechargeable non amovible de 3,7 V, 1000 mAh, qui peut être chargée pendant que la console est en
            mode repos. Elle pèse 210 g. Le logo Sony n'y est plus puisque nous avons à la place un pavé tactile. Le dessus de la manette comporte une barre lumineuse
            avec trois LED qui s'allument de différentes couleurs pouvant être utilisées pour identifier les joueurs en faisant correspondre les couleurs des personnages
            qu'ils contrôlent dans un jeu, pour fournir une rétroaction améliorée ou une immersion en changeant les motifs ou les couleurs en réaction au gameplay - Grand
            Theft Auto V, lorsque le joueur est recherché par la police, la barre lumineuse clignote en rouge et bleu -. Elle est utilisée en conjonction avec la caméra
            PlayStation pour juger les positions et les mouvements de plusieurs joueurs. Le contrôleur comporte plusieurs connecteurs d'entrée et de sortie : une prise
            casque stéréo, un port micro-USB et un port d'extension. Elle peut être chargé en utilisant une station de charge dédiée, via microUSB en utilisant un
            chargeur autonome, ou via la console même lorsque celle-ci est éteinte. Elle comprend un haut-parleur mono, comme la télécommande Wii, et est la deuxième
            manette de l'histoire du jeu vidéo à avoir une telle fonctionnalité. La DualShock 4 comporte les boutons suivants : bouton PS, SHARE, OPTIONS, boutons
            directionnels, boutons d'action - triangle, cercle, croix, carré -, boutons d'épaule - R1 / L1 -, déclencheurs - R2 / L2 -, boutons de clic analogiques -
            L3 / R3 - et pavé tactile. Ceux-ci marquent des changements majeurs par rapport à la DualShock 3 et ses prédécesseurs. Les boutons START et SELECT sont
            fusionnés en un seul bouton OPTION. Un bouton SHARE dédié permet aux joueurs de télécharger des captures d'écran et des vidéos de leurs expériences de jeu.
            Les joysticks et les déclencheurs sont repensés avec la surface striée des joysticks comportant désormais un anneau extérieur entourant les capuchons de dôme
            convexes. Les boutons L1, L2, R1 et R2 sont sensibles à la pression, un changement par rapport à la fonctionnalité des DualShock 2 et 3. La PlayStation 3 est
            compatible avec la DualShock 4 via un câble microUSB ou, depuis 2019, sans-fil - Allez dans le menu <em>Paramètres des accessoires</em>, Sélectionnez
            <em>Gérer les périphériques Bluetooth</em> puis <em>Enregistrer un nouvel appareil</em>. Lorsque la PS3 lance la recherche, appuyez simultanément sur les
            boutons PS et Share de la manette DualShock 4. Votre PS3 devrait reconnaître un contrôleur sans-fil et vous pourrez alors jouer avec votre manette PS4. Notez
            qu'il vous sera impossible d'allumer la console si elle est éteinte et qu'il reste des limitations lors de l'utilisation de votre DualShock 4 comme l’absence
            de la fonction vibration, du capteur de mouvement et du bouton PS.
        </p>
        <h3>DualSense</h3>
        <figure>
            <a href="../../static/img/sony/dual_next.jpg"><img src="../../static/img/sony/dual_next_mini.png" alt="DualSense" title="DualSense vs DualShock 4"></a>
            <a href="../../static/img/sony/dualSense.jpg"><img src="../../static/img/sony/dualSense_mini.png" alt="DualSense" title="La DualSense pour PS5"></a>
            <a href="../../static/img/sony/dualSense_profile.jpg"><img src="../../static/img/sony/dualSense_profile_mini.png" alt="DualSense"
                                                                         title="La DualSense de profil"></a>
        </figure>
        <p>
            La DualSense est le contrôleur de la PlayStation 5 et dévoilé le 7 avril 2020. Il est basé sur le contrôleur DualShock 4 qui l'a précédé mais avec une
            évolution de sa conception et de ses capacités influencées par les discussions avec les concepteurs de jeux et les joueurs. Contrairement aux précédents
            contrôleurs DualShock, la DualSense a un schéma de coloration bicolore - principalement blanc avec face noire - et des boutons d'action monochromes. Il
            incorpore un design plus ergonomique qui est un peu plus grand et plus rond que le DualShock 4. La barre lumineuse est déplacée en haut du contrôleur vers les
            bords gauche et droit du pavé tactile. Alors que le contrôleur maintient le même nombre de boutons que la DualShock 4, le bouton <em>Share</em> est remplacé
            par <em>Create</em> avec un accent étendu sur la création de contenu à partager avec les autres. La DualSense a un fort retour haptique grâce à des
            actionneurs à bobine mobile, qui sont destinés à donner un meilleur retour en jeu. Le haut-parleur du contrôleur est amélioré et est maintenant augmenté par
            un nouveau réseau de microphones intégré permettant aux joueurs de parler aux autres en utilisant uniquement la manette. Le contrôleur a des déclencheurs
            adaptatifs qui peuvent changer la résistance au joueur si nécessaire, soutenant une expérience telle que bander virtuellement une flèche sur un arc. La
            connectivité comprend une prise audio 3,5 mm et USB-C, qui remplace le port microUSB de la DualShock 4. Sa batterie est améliorée pour une capacité de 1560
            mAh contre 1000 pour la DualShock 4. Une fuite Twitter du 12 août 2020 provenant de Galaxy666 annonce que
            <cite>
                &#10077;la DualSense est bien plus agréable et confortable que la DualShock 4, les réactions sont magiques et les structures mécaniques des gâchettes
                semblent complexes, les gens ne seront pas déçus&#10080;
            </cite>.
        </p>

        <h2 id="easter">Easter Egg</h2>
        <p>
            Un easter egg - littéralement 'œuf de Pâques' - est, en informatique ou dans les jeux vidéo, une fonction cachée au sein d'un programme - une image animée, un
            jeu, un message électronique... - accessible grâce à un mot-clé ou à une combinaison de touches ou de clics. Ce n'est pas un virus dans le sens où il n'abîme
            ni le programme original ni les autres données et ne se propage pas. Il est caché et inséré volontairement par les créateurs. Il peut s'agir de la liste des
            auteurs du logiciel, d'un jeu, d'une blague, d'une séquence inédite, de bruitages. Certains se trouvent aussi dans les jeux vidéo - Diablo est un exemple
            connu -. Les niveaux secrets des jeux vidéo sont souvent classés comme easter eggs, ainsi que les clins d'œil ou les références.
        </p>
        <h3>L'origine de l'Easter Egg</h3>
        <p>
            Les références cachées existent dans des œuvres depuis longtemps, mais l'habitude de les appeler easter egg proviendrait du film The Rocky Horror Picture Show
            sorti en 1975 car des acteurs avaient caché des œufs de Pâques sur le plateau, mais ne les avaient pas tous trouvés. Quelques-uns sont encore visibles à
            l'écran dans le montage final du long métrage. La chasse aux œufs de Pâques au sens littéral dans ce film est reprise ensuite pour désigner les références
            cachées par d'autres réalisateurs.
            <br>
            Par la suite, d'autres œuvres reprennent ce terme, lorsqu'elles traitent de ce sujet de manière secondaire ou quand cela fait partie de l'intrigue principale.
            L'autre origine est l'utilisation du terme easter egg provienne du jeu "Adventure", sorti en 1979, sur Atari 2600 pour décrire une fonctionnalité ou un
            message caché, développé par l'employé Warren Robinett. À cette époque, Atari n'incluait pas les noms des développeurs dans les génériques de jeu, par peur de
            se faire voler leurs employés. Robinett, en désaccord avec ses superviseurs sur ce manque de reconnaissance, a secrètement inséré le message
            <cite>
                &#10077;Créé par Warren Robinett&#10080;
            </cite>
            dans le jeu. Ce message apparaît seulement lorsque le joueur déplace son avatar sur un pixel spécifique - le Gray Dot -, situé dans une certaine partie du jeu,
            permettant ainsi d'entrer dans une partie cachée du monde où le générique peut être lu. Peu après son départ, le message fut découvert par un joueur, qui
            partagea sa découverte avec Atari. Atari voulut supprimer le message du jeu, mais cet effort fut jugé trop coûteux. Par la suite, Steven Wright, le directeur
            chez Atari, suggéra de conserver le message et même d'encourager l'inclusion de tels messages dans les prochains jeux, en les décrivant comme des easter egg
            que les joueurs devraient trouver.
        </p>

        <h2 id="esrb">Entertainment Software Rating Board</h2>
        <p>
            L'ESRB - <em>Entertainment Software Rating Board</em> - est une organisation d'autorégulation américaine qui attribue des classifications d'âge et de contenu
            aux jeux vidéo grand public. L'ESRB est créé en 1994 par l'<em>Entertainment Software Association</em> en réponse aux critiques de jeux vidéo controversés à
            contenu excessivement violent ou sexuel, en particulier après les audiences du Congrès de 1993 à la suite de la commercialisation de "Mortal Kombat" et
            "Night Trap". L'industrie, sous la pression d'une éventuelle surveillance gouvernementale, établit la création d'un système de classification volontaire basé
            sur le système de classification des films de la <em>Motion Picture Association of America</em> avec des considérations supplémentaires pour l'interactivité
            des jeux vidéo. Le conseil attribue des évaluations aux jeux en fonction de leur contenu, des systèmes de classification des films utilisés dans de nombreux
            pays, d'une combinaison de six niveaux basés sur l'âge destinés à aider les consommateurs à déterminer le contenu et la pertinence d'un jeu, avec un système
            de descripteurs de contenu qui détaillent le contenu présent dans un jeu.
            <br>
            Les notes sont déterminées par un questionnaire, des séquences vidéo du jeu et par un examen d'un panel de critiques qui lui attribuent une note. Les
            évaluations sont destinées aux parents afin qu'ils puissent prendre des décisions éclairées sur l'achat de jeux pour leurs enfants. Une fois qu'un jeu est
            évalué, l'ESRB maintient un code d'éthique pour la publicité et la promotion des jeux vidéo, garantissant que le matériel de marketing pour les jeux est
            destiné aux publics appropriés.
            <br>
            Malgré son accueil positif, l'ESRB subit les critiques des politiciens et d'autres groupes de surveillance pour la structure de ses opérations, en particulier
            à la suite d'un incident en 2005 qui entoura la gestion par l'organisation de contenu caché et  répréhensible dans "Grand Theft Auto: San Andreas" qui peut
            être accessible à l'aide d'une modification créée par l'utilisateur. L'ESRB est accusé d'avoir un conflit d'intérêts en raison de son lien direct dans
            l'industrie du jeu vidéo, et qu'il ne note pas certains jeux assez durement, comme la série "Grand Theft Auto", pour leur contenu violent ou sexuel afin de
            protéger leur viabilité commerciale. Au contraire, d'autres critiques font valoir que l'ESRB attribue une note trop élevée à certains jeux pour leur contenu
            et que son influence étouffe la viabilité des jeux vidéo destinés aux adultes en raison des restrictions du conseil d'administration sur la manière dont ils
            sont commercialisés et vendus. En gros, ils font leur taf et il y aura toujours des esprits chagrins pour venir couiner...
        </p>

        <h2 id="fellow">Fellow</h2>
        <p>
            Le titre de fellow est un titre honorifique attribué par une institution à une personnalité méritante, élue ou invitée. Ce titre se rencontre dans le monde
            académique, les institutions universitaires, sociétés savantes ou professionnelles et également dans des sociétés privées à caractère technologique. cette
            distinction n'implique pas de lien salarial ni d'obligation de travail particulière. Les fellows font partie d'un groupe de chercheurs qui coopèrent dans la
            poursuite de leur recherche. Les fellows peuvent aussi comprendre des professeurs invités, des chercheurs post-doctoraux ou des doctorants. Le titre peut
            également indiquer une forme de bourse - fellowships - attribuée à des individus de haut niveau scientifique. De fait, le sens du terme fluctue selon les
            circonstances.
            <br>
            Un certain nombre d'entreprises ont créé des postes particuliers, destinés à des personnalités scientifiques de très haut niveau, qui jouissent en général
            d'une large autonomie et d'importants moyens pour poursuivre leur recherche. Les premières sociétés sont IBM, Intel, Sun Microsystems, Bell Labs, Microsoft,
            Google et Apple en technologies de l'information et de la communication, Boston Scientific pour les appareils médicaux entre autres. Ce poste, qui est le
            grade le plus élevé que l'on peut atteindre dans une carrière technique, est aussi nommé technical fellow. Le grade de fellow est aussi le grade le plus élevé
            dans certaines professions réglementées, comme les arbitres du <em>Chartered Institute of Arbitrators</em> ou les chirurgiens du Royal College of Surgeons. Le
            Conseil de l'Institut canadien des ingénieurs élit annuellement au rang de fellow un certain nombre d'ingénieurs pour leur excellence et pour les services
            rendus. De même, on peut être fellow de l'Institut canadien des actuaires. On retrouve le fellowship en France principalement dans le domaine médical.
        </P>

        <h2 id="fds">Famicom Disk System</h2>
        <p>
            Le <em>Famicom Disk System</em> est un périphérique de la console Famicom. Vendu par Nintendo, il se place sous la console et utilise un format de disquette
            permettant de stocker des jeux. En cette fin d'année 1984, Nintendo réfléchit à l'après-Famicom, car elle n'imagine pas que sa console puisse rester viable
            commercialement encore de longues années. Le principal problème lié à la première console de salon est sa capacité de stockage mémoire et Nintendo décide donc
            d'y remédier en élaborant une extension matérielle permettant de lire des disquettes, lesquelles disposent d'une plus grande capacité de mémoire et en outre
            offrent la possibilité de sauvegarder des parties en cours.
            <br>
            Les disquettes du Famicom Disk System sont ré-inscriptibles, c'est-à-dire que l'on peut acheter un jeu sur format disquette mais, une fois le jeu fini, on
            télécharge sur sa disquette un nouveau jeu au choix via une borne spéciale, le <em>Disk Writer</em>. Cette borne spéciale est disponible dans de nombreux
            centres commerciaux et magasins du Japon. De grandes séries Nintendo telles que "Super Mario", "Metroid" ou "The Legend of Zelda" sont adaptées sur le Famicom
            Disk System.
        </p>

        <h2 id="firmware">Firmware</h2>
        <p>
            Dans un système informatique, un firmware est un programme intégré dans un matériel informatique - ordinateur, photocopieur, disque dur, routeur, appareil
            photo numérique... - pour qu'il puisse fonctionner. Le firmware permet à un matériel informatique d’évoluer via des mises à jour, d'intégrer de nouvelles
            fonctionnalités sans avoir besoin de revoir complètement le design du hardware.
            <br>
            La mémoire dans laquelle réside le firmware est soit non volatile, stockant le programme et les données même lorsqu'elle n'est pas alimentée en électricité -
            cas le plus courant, avec des ROM, des EPROM ou une mémoire flash - soit volatile, donc effacée lorsqu'elle n'est plus alimentée en électricité. Dans ce cas,
            le firmware doit être chargé par un pilote à la mise sous tension, ce qui est peu pratique. Dans la plupart des cas, ce logiciel gère le fonctionnement
            interne du système électronique. D'une manière générale, le firmware cumule les avantages du logiciel, qu'il est possible de modifier, et du matériel, plus
            efficace. De son côté, le firmware interagit avec des composants matériels qui ne peuvent plus être modifiés une fois fabriqués, ce qui réduit la nécessité de
            le mettre à jour.
        </p>

        <h2 id="gdrom">GD-Rom</h2>
        <p>
            Ce matériel est similaire à la norme CD-ROM, excepté au niveau des microcuvettes du disque qui sont beaucoup plus resserrées, ce qui donne une plus grande
            capacité de stockage : environ 1,2 gigaoctets soit le double de la capacité de stockage d'un CD-ROM standard. Il est développé pour Sega par la société
            Yamaha. Le GD-ROM est également commercialisé en tant que mise à niveau du Naomi, ainsi que le Naomi 2, permettant l'utilisation d'un autre média que la
            cartouche d'origine. Je vous invite à découvrir ce très bel explicatif sur le
            <a href="https://www.youtube.com/watch?v=ws5weL2CkIE" title="Naomi">système d'arcade Naomi</a>.
        </p>

        <h2 id="gpu">GPU</h2>
        <p>
            Une unité de traitement graphique - <em>Graphic Physical Unit</em> - est un circuit électronique spécialisé conçu pour manipuler et modifier rapidement la
            mémoire afin d'accélérer la création d'images dans un tampon de trame destiné à la sortie vers un dispositif d'affichage. Les GPU s'utilisent dans les
            systèmes embarqués, les téléphones mobiles, les ordinateurs personnels, les stations de travail et les consoles de jeux. Les GPU modernes sont très efficaces
            pour manipuler les graphiques informatiques et le traitement d'images. Leur structure hautement parallèle les rend plus efficaces que les unités centrales de
            traitement - CPU - à usage général pour les algorithmes qui traitent de gros blocs de données en parallèle. Dans un ordinateur personnel, un GPU peut être
            présent sur une carte vidéo ou intégré à la carte mère. Dans certains processeurs, ils sont intégrés à la puce du processeur. Le terme <em>GPU</em> est
            inventé par Sony en référence au GPU Sony conçu par Toshiba pour la console PlayStation en 1994. Le terme est popularisé par Nvidia en 1999, qui commercialise
            la GeForce 256 comme premier GPU au monde. Il est présenté comme un processeur monopuce avec moteurs intégrés de transformation, d'éclairage, de
            configuration/découpage de triangle et de rendu. <em>Rival ATI Technologies</em> invente le terme <em>unité de traitement visuel</em> ou VPU avec la sortie de
            la Radeon 9700 en 2002.
        </p>

        <h2 id="hardware">Hardware</h2>
        <p>
            Un matériel informatique - hardware - est une pièce détachée d'un appareil informatique. Il s'agit d'un domaine important de l’informatique qui va de pair
            avec le logiciel - software ou firmware. Les pièces situées à l'intérieur de l'appareil sont indispensables à son fonctionnement et celles situées à
            l'extérieur - les périphériques - sont considérées secondaires. Les pièces intérieures sont montées sur des circuits imprimés. Différentes pièces sont
            construites par différentes marques et connectées entre elles. Le respect des normes par les différentes marques permet le fonctionnement de l'ensemble. Les
            pièces servent soit à recevoir des informations, à les envoyer, les échanger, les stocker ou les traiter. Toutes les opérations sont effectuées conformément
            aux instructions contenues dans les logiciels et aux manipulations des périphériques de l'interface homme-machine.
        </p>

        <h2 id="hdmi">HDMI</h2>
        <p>
            Le <em>High Definition Multimedia Interface</em> - Interface Multimédia Haute Définition - est un standard et interface audio/vidéo totalement numérique pour
            transmettre des flux chiffrés constitués de données vidéo non compressées et des données audio pouvant être compressées destiné au marché grand public. Le
            HDMI permet de relier une source audio/vidéo DRM par câble cuivre ou fibre – comme un lecteur Blu-ray, un ordinateur ou une console de jeu – à un dispositif
            compatible – tel un téléviseur HD, un vidéoprojecteur ou un casque de réalité virtuelle.
        </p>

        <h2 id="vector">Image vectorielle</h2>
        <p>
            Une image vectorielle est une image numérique composée d'objets géométriques individuels, de primitives géométriques - segments de droite, arcs de cercle,
            courbes de Bézier, polygones... -, définis chacun par différents attributs - forme, position, couleur, remplissage, visibilité... - et auxquels on peut
            appliquer différentes transformations - homothéties, similitude, rotations, écrasement, mise à l'échelle, extrusion, inclinaison, effet miroir, dégradé de
            formes, morphage, symétrie, translation, interpolation, coniques ou bien les formes de révolution... Tous ces termes sont à apprendre par coeur bien
            évidemment -. Elle se différencie en cela des images matricielles - bitmap -, qui sont constituées de pixels... Bon ! OK! Concrètement Une image vectorielle
            est redimensionnable sans perte de qualité, contrairement à une image matricielle... Haaaaa !
        </p>

        <h2 id="kinect">Kinect</h2>
        <p>
            Kinect - nom de code <em>Project Natal</em> - est un périphérique destiné à la Xbox 360 pour le V1 et la Xbox One et Windows pour le V2 permettant de
            contrôler une interface sans utiliser de manette et conçu par Microsoft en septembre 2008. <em>Kinect</em> est issu des mots anglais kinetic - cinétique - et
            connect - connecter -. Deux mois après sa sortie soit le 5 janvier 2011, Microsoft annonce avoir vendu 8 millions de Kinect, dont un million en seulement 10
            jours. Kinect entre au Livre Guinness des records comme étant l'accessoire high-tech le plus vendu dans un court laps de temps avec 10 millions d'unités
            vendues, soit une moyenne de 130 000 Kinect vendus chaque jour à travers le monde. Les ventes se sont ensuite érodées, jusqu'à ce que Microsoft décide de ne
            pas intégrer de port Kinect dans la Xbox One S sortie en août 2016. Le fabricant annonce l'arrêt du développement et de la production de Kinect en octobre
            2017.
            <br>
            La Kinect est une caméra utilisant des techniques d'interaction développées par la société israélienne PrimeSense. Elle est basée sur un périphérique d’entrée
            branché sur la console Xbox 360 qui permet d'interagir par commande vocale, reconnaissance de mouvement et d'image. On peut ainsi jouer sur des jeux
            spécialement développés pour le projet, sans aucune manette ni périphérique autre que son propre corps. Cette détection de mouvements sans périphériques est
            déjà présente dans l’EyeToy de Sony pour la PS2. Cette technologie est destinée au grand public avec une expérience proche des party games de la Wii. La
            technique est annoncée lors du salon de l’<a href="https://www.youtube.com/watch?v=c326ausKUHE" title="E3 2009">E3 2009</a>. Microsoft dévoile la première
            publicité officielle de son nouveau périphérique le 21 octobre 2010, montrant à quel point la firme vise le grand public. Le géant de l'informatique confirme
            dépenser plus de 425 millions d'€ en marketing pour s'assurer de toucher au maximum la population - Il pouvait me les envoyer directement, j'aurai été bien
            touché ! -. En ajoutant les 425 millions du lancement du Windows Phone 7, l'entreprise va ainsi débourser au total plus d'1 milliard uniquement pour de la
            communication en 2 mois... pfff ! Quel gâchis pour l'Homme ! Initialement hostile au portage du Kinect sur les PC, la société Microsoft change d'avis et
            l'introduit sur Windows le 1er février 2012. Cette version inclut de nouvelles fonctionnalités : Le capteur est repensé pour fonctionner à partir d’une
            distance de 50 cm.
        </p>

        <h2 id="laserdisc">LaserDisc</h2>
        <p>
            Le LaserDisc est le premier support de stockage optique à être commercialisé en 1978 aux Etats-Unis sous le nom de MCA DiscoVision. Bien qu’il offre une
            meilleure qualité de son et d’image par rapport aux supports de l'époque - notamment les cassettes VHS et Betamax -, le LaserDisc ne connait que peu de
            succès, en raison du prix élevé des lecteurs et du fait qu’il ne permet pas d’enregistrer les programmes de télévision. S’il est, dès son introduction,
            plébiscité par les détenteurs de Home-Cinema, c’est seulement en Asie - Hong Kong, Malaisie et Singapour -, dans les années 90, que le LaserDisc s’est
            véritablement diffusé dans les foyers. C’est à partir de la technologie du LaserDisc que sont élaborés plusieurs supports de stockage optique, notamment les
            CD et DVD.
        </p>

        <h2 id="microc">Microcontrôleur</h2>
        <p>
            Un microcontrôleur est un circuit intégré qui rassemble les éléments essentiels d'un ordinateur : processeur, mémoires - mémoire morte et mémoire vive -,
            unités périphériques et interfaces d'entrées-sorties. Les microcontrôleurs se caractérisent par un plus haut degré d'intégration, une plus faible consommation
            électrique, une vitesse de fonctionnement plus faible de quelques mégahertz jusqu'à plus d'un gigahertz et un coût réduit par rapport aux microprocesseurs
            polyvalents utilisés dans les ordinateurs personnels. Par rapport à des systèmes électroniques à base de microprocesseurs et autres composants séparés, les
            microcontrôleurs permettent de diminuer la taille, la consommation électrique et le coût des produits. Ils permettent de démocratiser l'utilisation de
            l'informatique dans un grand nombre de produits et de procédés. Les microcontrôleurs sont fréquemment utilisés dans les systèmes embarqués, comme les
            contrôleurs des moteurs automobiles, les télécommandes, les appareils de bureau, l'électroménager, les jouets, la téléphonie mobile...
        </p>

        <h2 id="microp">Microprocesseur</h2>
        <p>
            Un microprocesseur est un processeur dont tous les composants sont suffisamment miniatures pour être regroupés dans un unique boitier. Fonctionnellement, le
            processeur est la partie d’un ordinateur qui exécute les instructions et traite les données des programmes. Jusqu’au début des années 70, les différents
            composants électroniques, nécessaires au fonctionnement d'un processeur ne peuvent pas tenir sur un seul circuit intégré, ce qui nécessite d'interconnecter de
            nombreux composants dont plusieurs circuits intégrés. La société américaine Intel réussit pour la première fois à placer tous les composants qui
            constituent un processeur sur un seul circuit intégré donnant ainsi naissance au microprocesseur en 1971.
            <br>
            Cette miniaturisation permet d'augmenter les vitesses de fonctionnement des processeurs grâce à la réduction des distances entre les composants, de réduire
            les coûts grâce au remplacement de plusieurs circuits par un seul, d'augmenter la fiabilité - en supprimant les connexions entre les composants du processeur,
            on supprime l'un des principaux vecteurs de panne -, de créer des ordinateurs bien plus petits - les micro-ordinateurs - et de réduire la consommation
            énergétique.
        </p>

        <h2 id="windows">Microsoft Windows</h2>
        <p>
            Microsoft Windows, communément appelé Windows, est un groupe de plusieurs familles de systèmes d'exploitation graphiques propriétaires qui sont toutes
            développées et commercialisées par Microsoft. Chaque famille s'adresse à un certain secteur de l'industrie informatique. Les familles Microsoft Windows
            comprennent Windows NT et Windows IoT, ceux-ci englobent des sous-familles, Windows Server ou Windows Embedded Compact - Windows CE -. Les anciennes familles
            comprennent Windows 9x, Windows Mobile et Windows Phone.
            <br>
            Microsoft introduit cet environnement d'exploitation nommé Windows le 20 novembre 1985, en tant que shell de système d'exploitation graphique pour MS-DOS en
            réponse à l'intérêt croissant pour les interfaces utilisateur graphiques - GUI -. Microsoft Windows domine le marché mondial des ordinateurs personnels -
            Personnal Computer - avec plus de 90% de part de marché, dépassant Mac OS introduit en 1984. Apple en est venu à voir Windows comme une épine sous le pied sur
            leur innovation dans le développement de l'interface graphique comme le Lisa et Macintosh, finalement réglé au tribunal en faveur de Microsoft en 1993.
            Cependant, en 2014, Microsoft admet avoir perdu la majorité du marché global des systèmes d'exploitation au profit d'Android, en raison de la croissance
            massive des ventes de smartphones Android. Le nombre d'appareils Windows vendus est inférieur à 25% de celui des appareils Android vendus en 2014. Cette
            comparaison s'arrête là cependant car les deux systèmes d'exploitation ciblent des plates-formes différentes. La version la plus récente de Windows pour
            les PC, les tablettes et les périphériques embarqués est Windows 10 depuis février 2020. La version la plus récente pour les ordinateurs serveurs est Windows
            Server, version 2004. Une version spécialisée de Windows fonctionne également sur la console de jeu vidéo Xbox One.
        </p>

        <h2 id="modding">Modding</h2>
        <p>
            Le modding est une expression d'argot dérivée du verbe modifier. Modding fait référence à l'acte de modifier du matériel, des logiciels ou pratiquement toute
            autre chose, pour exécuter une fonction qui n'est pas initialement conçue ou prévue par le concepteur, ou pour atteindre une spécification sur mesure. Le
            terme modding est utilisé au sein de la communauté des jeux vidéo, notamment en ce qui concerne la création de contenu nouveau ou modifié et le partage de
            celui-ci via le Web. Il peut être appliqué à l'overclocking des ordinateurs afin d'augmenter la fréquence à laquelle le processeur fonctionne. Le modding de
            cas est une activité populaire parmi de nombreux passionnés d'informatique qui implique la personnalisation d'un boîtier d'ordinateur ou l'installation d'une
            technologie de refroidissement par eau. En ce qui concerne les automobiles, le modding peut signifier le réglage du moteur, le remappage de l'unité de
            commande du moteur d'un véhicule ou la personnalisation de la carrosserie.
        </p>

        <h2 id="model1">Model 1</h2>
        <p>
            Le Model 1, commercialisé en 1992, est une amélioration du System 32 de Sega. Ce système et le jeu "Virtua Racing" n'est pas le premier jeux à 3D polygonée
            puisqu'Atari a déjà tenté l'expérience avec <a href="https://www.youtube.com/watch?v=ofUj8rIgG4U" title="Hard Drivin'">Hard Drivin'</a> - Cette vidéo présente
            "Racing Drivin'" qui est la version arcade -. Sega connait la complexité de ce matériel et la difficulté que la société américaine rencontre pour le réaliser.
            Mais le désir de Sega n'est pas de commercialiser ce système, seulement de voir si fabriquer un système d'arcade capable de proposer de la 3D polygonée est
            viable, réalisable et rentable. Sega se rapproche donc de <em>General Electric Aerospace</em> pour se faire aider dans la création du système gérant la 3D.
            <em>General Electric Aerospace</em> fabriqua les tout premiers simulateurs en 3D pour la NASA dans les années 60.
            <br>
            La tache s'avère bien moins compliquée que pour Atari et son "Hard Drivin'". Le Model 1 voit le jour avec le premier jeu maintenant culte,
            <a href="https://www.youtube.com/watch?v=XdPc6b8-9UU" title="Virtua Racing">Virtua Racing</a>, développé en même temps que le système. Le succès en interne de
            ce jeu est tel que Sega se décide malgré tout à le commercialiser. Le coût exorbitant de la production de ce système limite le développement et la production
            à seulement 7 jeux, parmi eux le célèbre jeu de combat <a href="https://www.youtube.com/watch?v=fn7PPPd5NqA" title="Virtua Fighter">Virtua Fighter</a>.
        </p>

        <h2 id="nfc">NFC</h2>
        <p>
            La Communication en Champ Proche, <em>Near Field Communication</em> - NFC -, est une technologie de communication sans fil à courte portée et à haute
            fréquence, permettant l'échange d'informations entre des périphériques jusqu'à une distance d'environ 10 cm. Cette technologie est une extension de la norme
            ISO/CEI 14443 standardisant les cartes de proximité utilisant la radio-identification - RFID - qui combinent une carte à puce et un lecteur au sein d'un seul
            périphérique. Un périphérique NFC est capable de communiquer avec des équipements ISO/CEI 14443, avec un autre périphérique NFC ou avec certaines
            infrastructures sans-contact comme les composteurs des transports en commun ou les terminaux de paiement chez les commerçants. Cette technologie équipe des
            cartes et des lecteurs utilisés dans les transports, dans le commerce ou pour l’accès à certains services publics, et la NFC équipe de plus en plus de
            terminaux mobiles. La NFC équipe en effet 50 millions de tablettes tactiles ou téléphones mobiles en 2011. Dotés d’un écran, d’un clavier et d’une connexion
            internet, ces terminaux NFC ont un fort potentiel d’usages en favorisant les interactions entre les machines, les objets et un contexte.
        </p>

        <h2 id="nod">Nintendo Optical Discs</h2>
        <p>
            Les disques optiques Nintendo sont des supports physiques utilisés sur trois des consoles de Nintendo qui suivent la Nintendo 64. Il s'agit du disque de jeu
            GameCube, du disque optique Wii et Wii U. La taille physique d'un disque de jeu GameCube est celle d'un miniDVD. Les disques optiques Wii et Wii U ont la même
            taille que les DVD, CD et Blu-ray. Pour maintenir la rétrocompatibilité entre les générations de consoles de jeux, les disques GameCube sont compatibles avec
            le premier modèle de la Wii, et les disques optiques Wii sont compatibles avec la Wii U. Nintendo abandonne les supports sur disque en 2017 au profit des
            cartes de jeu pour le successeur de la Wii U, la Switch.
        </p>
        <h3>GameCube Game Disc</h3>
        <p>
            Le GameCube Game Disc est le support de jeu pour GameCube créé par Panasonic et étendu à une utilisation dans le mode de rétrocompatibilité du premier modèle
            de Wii. La GameCube est la première console de disque optique de Nintendo après les platesformes basées sur des cartouches ROM. Le disque possède un stockage
            de 1,46 Go et une taille de 8 cm - miniDVD -. Il est choisi par Nintendo pour empêcher la violation du droit d'auteur de ses jeux, pour réduire les coûts de
            fabrication par rapport aux Nintendo 64 Game Paks et pour éviter les frais de licence du format DVD. Les disques de jeu GameCube n'utilisent pas le système
            Content Scramble que l'on trouve sur les disques DVD-Vidéo normaux, car Nintendo n'est pas satisfait de son niveau de sécurité. La GameCube ne peut pas être
            utilisé comme un lecteur DVD courant, à l'exception du Panasonic Q. Certains jeux avec de grandes quantités de données couvrent deux disques, tels que
            "Resident Evil 4", "Enter the Matrix" et "Tales of Symphonia". Certains jeux multiplateformes qui tiennent sur des disques uniques pour PlayStation 2 et Xbox
            ont certaines fonctionnalités supprimées afin de tenir sur les disques de jeu GameCube. Les FMV et les clips audio ont une compression plus élevée ou une
            qualité inférieure pour tenir sur un seul disque.
        </p>
        <h3>Wii Optical Disc</h3>
        <p>
            Le disque optique Wii est aussi créé par Panasonic. Nintendo étend sa technologie propriétaire pour utiliser un disque DVD pleine taille de 12 cm et d'une
            capacité de stockage de 8,54 Go en conservant les avantages du GameCube Game Disc et en ajoutant la capacité standard d'un DVD-ROM double couche. Les disques
            Wii peuvent inclure des mises à jour du logiciel du système Wii, qui sont installées avant de démarrer le jeu, garantissant que les systèmes non connectés à
            Internet sont toujours mis à jour. Pour les mêmes raisons que GameCube et Wii U, la Wii ne peut pas lire de films DVD ou CD. La Wii peut utiliser des disques
            double couche mais tous les jeux restent monocouche jusqu'à la sortie de Super Smash Bros. Dès lors, Nintendo admet que certaines consoles Wii peuvent avoir
            du mal à lire les disques double couche en raison d'une lentille laser sale. Nintendo réparera d'ailleurs les systèmes défaillants et sortira un kit de
            nettoyage que les utilisateurs pourront se procurer.
        </p>
        <h3>Wii U Optical Disc</h3>
        <p>
            Le disque optique Wii U a une capacité de 25 Go. Le logiciel du système Wii U est rétrocompatible avec les disques optiques Wii, mais pas avec les disques de
            jeu GameCube. Ils diffèrent en apparence de la plupart des autres disques optiques en ce qu'ils ont des bords doux et arrondis - Hou ! Guiliguili ! -. Le
            lecteur optique de la console est développé et fourni par Panasonic, membre fondateur de la <em>Blu-ray Disc Association</em>. Comme avec les Disques optiques
            GameCube et Wii, il est choisi par Nintendo pour empêcher la violation des droits d'auteur des jeux, pour réduire les coûts en évitant les frais de licence à
            la Blu-ray Disc Association - BDA - et pour réduire les temps de chargement. Cela empêche également la console d'être modifiée en un lecteur de films DVD ou
            Blu-ray.
        </p>

        <h2 id="opengl">Open GL</h2>
        <p>
            OpenGL - <em>Open Graphics Library</em> - est un ensemble normalisé de fonctions de calcul d'images 2D ou 3D lancé par Silicon Graphics en 1992. Cette
            interface de programmation est disponible sur de nombreuses plateformes où elle est utilisée pour des applications qui vont du jeu vidéo jusqu'à la CAO en
            passant par la modélisation. OpenGL permet à un programme de déclarer la géométrie d'objets sous forme de points, de vecteurs, de polygones, de bitmaps et de
            textures. OpenGL effectue ensuite des calculs de projection en vue de déterminer l'image à l'écran, en tenant compte de la distance, de l'orientation, des
            ombres, de la transparence et du cadrage. L'interface regroupe environ 250 fonctions différentes qui sont utilisées pour afficher des scènes
            tridimensionnelles complexes à partir de simples primitives géométriques. Du fait de son ouverture, de sa souplesse d'utilisation et de sa disponibilité sur
            toutes les platesformes, elle est utilisée par la majorité des applications scientifiques, industrielles ou artistiques 3D et certaines applications 2D
            vectorielles.
            <br>
            Cette bibliothèque est également utilisée dans l'industrie du jeu vidéo où elle est souvent en rivalité avec la bibliothèque de Microsoft : Direct3D. Une
            version nommée OpenGL ES est conçue spécifiquement pour les applications embarquées - téléphones portables, agenda de poche, consoles de jeux… -.
        </p>

        <h2 id="overclocking">Overclocking</h2>
        <p>
            L'overclocking est une manipulation permettant d'augmenter la fréquence du signal d'horloge d'un processeur au-delà de sa fréquence nominale afin d'augmenter
            les performances de l'ordinateur. Le processeur overclocké exécute plus d'instructions par seconde, ce qui diminue le temps d'exécution de certains
            programmes. En contrepartie, il chauffe plus, et peut mal réaliser certaines opérations du fait d'une trop haute température interne, ou d'une tension
            d'alimentation trop faible par rapport à sa fréquence, le rendant instable.
        </p>

        <h2 id="peritel">Péritel</h2>
        <p>
            La péritélévision ou Péritel, marque déposée comme Frigo, désigne un dispositif de liaison et un connecteur audio et vidéo grand public utilisé en Europe. Il
            permet une connexion simplifiée des appareils qui exploitent des signaux audio et vidéo analogiques au moyen d'un connecteur à 21 broches. Il est aussi connu
            sous le sigle SCART - <em>Syndicat des Constructeurs d'Appareils Radiorécepteurs et Téléviseurs</em> -.
            <br>
            Il existe trois types de connectiques péritélévision : la fiche femelle sur les appareils, le cordon mâle/mâle, et le cordon prolongateur. Des cordons
            spéciaux ou adaptateurs vers les connecteurs audio-vidéo différents - RCA, Ushiden, DIN… - peuvent également permettre de relier les appareils dépourvus de
            Péritélévision. Dépourvu d'obligation depuis juillet 2015 d'être sur un téléviseur, la péritélévision est remplacée par la connectique HDMI -
            <em>High Definition Multimedia Interface</em> -.
        </p>

        <h2 id="pong">Pong</h2>
        <p>
            "Pong" est un des premiers jeux vidéo d'arcade imaginé par l'Américain Nolan Bushnell et développé par Allan Alcorn, commercialisé par Atari en novembre 1972.
            Bien que d'autres jeux vidéo existent déjà, comme <a href="https://www.youtube.com/watch?v=b3BQsCCwo8w" title="Computer Space">Computer Space</a>, "Pong" est
            le premier à devenir populaire. Le jeu est inspiré du tennis de table en vue de dessus et chaque joueur s'affronte en déplaçant la raquette virtuelle de haut
            en bas, via un bouton rotatif, de façon à garder la balle dans le terrain de jeu. Le joueur peut changer la direction de la balle en fonction de l'endroit où
            celle-ci tape sur la raquette alors que sa vitesse augmente graduellement au cours de la manche. Un score est affiché pour la partie en cours et des bruitages
            accompagnent la frappe de la balle sur les raquettes.
            <br>
            "Pong" est à l'origine un exercice demandé par Bushnell à Alcorn en guise d'entraînement. Une version similaire est créée par Ralph Baer pour la console
            Odyssey de Magnavox mais son existence reste peu connue. Surpris par la qualité du résultat, Bushnell et Ted Dabney, fondateurs d'Atari, décident de
            commercialiser le jeu dès 1972. La copie du concept entraîne d'ailleurs une poursuite en justice de Magnavox contre Atari pour violation de brevet en 1976.
            Mise sur le marché fin 72, la borne d'arcade est un succès : vendue à près de 8 000 exemplaires, Atari engrange 33,8 millions d'€ de chiffres d'affaires
            durant l'année 1975, dépassant toutes les prédictions de Bushnell et Dabney. Ce succès incita de nombreuses sociétés à se lancer dans le jeu vidéo en
            copiant le concept. "Pong" est porté sur console de salon dédiée, sous le nom Home Pong en 1975, commercialisée par Sears puis directement par Atari un an
            après. Ce double succès est considéré comme l'évènement précurseur de l'industrie du jeu vidéo avec une forte augmentation de l'offre, des centaines de
            consoles de salon reprenant le concept.
        </p>

        <h2 id="powerpc">PowerPC</h2>
        <p>
            PowerPC est une gamme de microprocesseurs dérivée de l'architecture de processeur RISC POWER d'IBM et développée conjointement par Apple, IBM et Freescale -
            anciennement Motorola Semiconducteurs -. Le rétro-acronyme de PowerPC est <em>Performance Optimization With Enhanced RISC Performance Computing</em>.
            L'architecture est gérée par la fondation Power.org depuis 2004 puis reprise par la Linux Foundation en 2019 qui place le design et les jeux d’instructions
            sous licence Open Source.
            <br>
            En micro-informatique, l'architecture PowerPC est surtout utilisée dans les Macintosh d'Apple de 1994 à 2006 et dans les serveurs d'IBM. Apple s'est tourné
            vers les processeurs x86 d'Intel depuis juin 2005. En effet, selon Steve Jobs, IBM ne parvient plus à produire des processeurs suffisamment performants et
            moins gourmands en énergie par rapport à la concurrence, notamment celle d'Intel. Selon d'autres sources officieuses, Steve Jobs refuse de payer à IBM les
            frais de recherche technologique pour créer de nouveaux PowerPC.
            <br>
            Les PowerPC sont aussi utilisés dans les consoles de jeux notamment le PowerPC 602 équipant un prototype de la console Pipp!n d'Apple en 1995. Un PowerPC
            Gekko, dérivé du G3, est utilisé dans la console GameCube. Un dérivé de ce processeur, le Cell est utilisé dans la PlayStation 3. Le PowerPC est enfin utilisé
            dans la Xbox 360 - Xenon -, la Nintendo Wii - Broadway - et la Nintendo Wii U - Espresso -.
        </p>

        <h2 id="powervr">PowerVR</h2>
        <p>
            PowerVR est avant tout une division d'Imagination Technologies qui développe du matériel et des logiciels pour le rendu 2D et 3D, ainsi que de l'encodage et
            décodage vidéo, le traitement d'image associé et l'accélération DirectX, OpenGL, OpenVG et OpenCL. PowerVR développe également des accélérateurs IA appelés
            <em>Neural Network Accelerator</em>. La gamme de produits PowerVR est lancée à l'origine pour concurrencer, sur le marché des PC de bureau, les accélérateurs
            matériels 3D avec un produit offrant un meilleur rapport qualité-prix que les produits existants comme ceux de 3dfx Interactive. L'évolution rapide de ce
            marché, notamment avec l'introduction d'OpenGL et de Direct3D, conduit à une consolidation rapide. Au fil du temps, cela se transforme en une série de
            conceptions qui peuvent être incorporées dans des architectures de système sur puce adaptées à l'utilisation d'appareils portables.
            <br>
            Les accélérateurs PowerVR ne sont pas fabriqués par PowerVR. Les conceptions de circuits intégrés et les brevets sont concédés sous licence à d'autres
            sociétés, telles que Texas Instruments, Intel, NEC, BlackBerry, Renesas, Samsung, STMicroelectronics, Freescale, Apple, NXP Semiconductors et bien d'autres
            ma bonne dame !
        </p>

        <h2 id="process">Processeur central</h2>
        <p>
            Un processeur ou Unité Centrale de Traitement - <em>Central Processing Unit</em>, CPU - est un composant présent dans de nombreux dispositifs électroniques
            qui exécute les instructions machine des programmes informatiques. Avec la mémoire, c'est notamment l'une des fonctions qui existent depuis les premiers
            ordinateurs. Un processeur construit en un seul circuit intégré est un microprocesseur.
            <br>
            L'invention du transistor en 1948 ouvre la voie à la miniaturisation des composants électroniques. Les premiers ordinateurs prennent la taille d'une pièce
            entière du fait de l'utilisation de lampes à vide volumineuses, grosses consommatrices d'énergie et génératrices calorifaires. Cette méthode coûteuse de
            conception pour une application conduit au développement de la production de masse de processeurs multi-usages. Cette tendance à la standardisation des
            ordinateurs centraux connait une accélération rapide avec l'avènement des circuits intégrés. Les circuits intégrés permettent la miniaturisation des
            processeurs. La miniaturisation et la standardisation permettront l'entrée de l'informatique dans les foyers.
        </p>

        <h2 id="zilog">Processeur Zilog 80</h2>
        <p>
            Le Zilog Z80 est un microprocesseur 8 bits dont l'une des particularités est le couplage de certains registres - 2x8 bits - et le bus d’adresses 16 bits
            permettant un traitement de l’information nettement plus rapide qu’avec un processeur 8 bits classique. Ce processeur est commercialisé pour la toute première
            fois en juillet 1976. Il devient très populaire au début des années 80 dans la conception des ordinateurs comme les Sinclair ZX80, ZX Spectrum, le standard
            MSX, les Amstrad CPC et plus tard dans les systèmes embarqués. Avec la famille des puces MOS 6502, il domine le marché des micros 8 bits à partir de la fin
            des années 70 jusqu’au milieu des années 80.
            <br>
            Les utilisations du processeur comprennent les calculatrices Texas Instruments, les consoles de jeux vidéo Master System, Game Boy et Game Gear. Certaines
            consoles plus puissantes dotées d’autres processeurs centraux comme la Neo Geo ou la Mega Drive avec son Motorola 68000, utilisent le Z80 comme processeur
            complémentaire afin de gérer le son ou les entrées/sorties du système.
            <br>
            Le Z80 n’est plus utilisé en 2007 que dans des systèmes embarqués, tels que des photocopieurs, fax, calculatrices et autres appareils de bureautique sous la
            forme de contrôleurs <em>tout en un</em> en raison de l’importante bibliothèque disponible pour ce processeur et la facilité de son interfaçage avec les
            claviers matriciels et les afficheurs LCD.
        </p>

        <h2 id="ay">Puce AY-3-8500</h2>
        <p>
            La puce AY-3-8500 <em>Ball & Paddle</em> est la première d'une série de circuits intégrés de General Instrument conçu pour le marché du jeu vidéo. Ces puces
            sont conçues pour la sortie vidéo à un modulateur, qui affiche le jeu sur un écran de télévision . La AY-3-8500 contient six jeux au choix - le tennis, le
            football, le squash, l'entrainement et deux jeux de tir. Introduit en 1976, Coleco devient le premier client à l'induire dans le développement d'une machine
            grâce à Ralph Baer. La AY-3-8500 est la première version et six autres seront développées de la vidéo noir et blanc jusqu'à la couleur pour l'AY-3-8515.
        </p>

        <h2 id="pla">Puce PLA</h2>
        <p>
            Un circuit logique programmable est un circuit intégré logique qui peut être reprogrammé après sa fabrication. Ce n'est pas une programmation au sens
            logiciel, contrairement à un microprocesseur, il n'exécute aucune ligne de code. C'est une reconfiguration où l'on modifie des connexions ou le comportement
            du composant, on connecte des portes logiques entre elles... Le verbe programmer est toutefois plus fréquent dans le sens de personnaliser. Il existe
            différents sigles en anglais dont PLA signifiant <em>Programmable Logic Array</em> - Tableau Logique Programmable.
            <br>
            L'architecture matérielle de ces réseaux sont des circuits composés de nombreuses cellules logiques élémentaires librement assemblables. Celles-ci sont
            connectées de manière définitive ou réversible par programmation afin de réaliser la ou les fonctions numériques voulues. L'intérêt est qu'une même puce peut
            être utilisée dans de nombreux systèmes électroniques différents. Certains modèles peuvent aussi comporter : de la mémoire d'usage général, des blocs DSP
            câblés, des boucles à verrouillage de phase pour la génération d'horloge.
        </p>

        <h2 id="sid">Puce SID</h2>
        <p>
            La SID - <em>Sound Interface Device</em> - est la puce gérant tout le son du Commodore 64 et 128. Cette puce est un véritable synthétiseur, dépassant
            technologiquement tout ce qui se fait à l'époque. Contribuant grandement au succès commercial du Commodore 64, elle est conçue et fabriquée par la filiale de
            Commodore International, MOS Technology. Elle reste utilisée dans les années 2000 notamment par l'Elektron SidStation, un instrument de musique électronique
            et par certains musiciens ou groupes tels que Machinae Supremacy.
        </p>

        <h2 id="tegra">Puce Tegra</h2>
        <p>
            La NVIDIA Tegra est un processeur tout en un - SoC - dérivé de la famille d'architecture ARM produit par NVIDIA. Il est destiné aux appareils mobiles comme
            les smartphones, PDA et MID. Le processeur est aussi présent dans les systèmes de navigation et de divertissement des automobiles, notamment chez Audi et le
            groupe Volkswagen. Le Tegra est un <em>ordinateur dans un circuit</em> qui intègre CPU, GPU, northbridge, southbridge et fonction de mémoire primaire dans un
            simple boîtier. Les premiers produits grand public à utiliser le Tegra sont disponibles depuis 2009.
            <br>
            La Nintendo Switch utilise un processeur NVIDIA Tegra personnalisé et Nintendo est d'ailleurs le seul à acheter ses puces chez NVidia, Sony et Microsoft se
            fournissant depuis longtemps chez AMD. D'ailleurs, la Switch étant sorti quelques années avant la PS5 et la Xbox Series, la puissance de sa Tegra commence
            à montrer ses limites. Ainsi, la Switch pourrait bénéficier d'une nouvelle technologie maison NVidia appelée  DLSS 2.0, la nouvelle génération de solutions
            graphiques et de technologies d'IA pour les consoles de jeu vidéo. Cette dernière - déjà inclue sur les GPU RTX - fait appel à une IA pour libérer de la
            puissance de calcul en optimisant certaines tâches, ou en utilisant une résolution variable sur certaines images. En activant le DLSS sur PC, on peut arriver
            à doubler le framerate sur certains jeux très gourmands, comme "Metro Exodus". D'ailleurs, le DLSS serait une technologie parfaite pour la firme de Kyoto, car
            cette dernière permet également de baisser la consommation électrique, ce qui est crucial pour une machine portable comme la Switch.
        </p>

        <h2 id="rfid">Radio-fréquence</h2>
        <p>
            Le terme radio-fréquence - RFID - désigne une fréquence d'onde électromagnétique située entre 3 kHz et 300 GHz incluant les fréquences utilisées par
            différents moyens de radiocommunication, notamment la téléphonie mobile, le Wi-Fi ou la radiodiffusion ainsi que des signaux destinés à d'autres usages comme
            les radars ou les fours à micro-ondes. Les ondes utilisant de telles fréquences sont les ondes radio. L'électronique dédiée au traitement des signaux RFID
            constitue un domaine bien particulier de l'électrotechnique qui couvre à la fois l'émission et la réception de ces signaux par des antennes et leur traitement
            analogique et/ou numérique mais aussi la conception physique des circuits, une particularité de ces ondes étant en effet de se propager à la fois dans les
            milieux conducteurs - câbles, composants... - mais aussi dans l'espace environnant.
        </p>

        <h2 id="secam">SECAM</h2>
        <p>
            Le SECAM, acronyme de <em>SEquentiel Couleur à Mémoire</em>, désigne un mode standardisé de codage vidéo analogique en couleurs, inventé par Henri de France
            et commercialisé à partir de 1967. Adapté aux formats vidéo 625 lignes et 25 images par seconde, le SECAM est implanté en France métropolitaine et DOM-TOM,
            dans les pays de l'Europe de l'est et en Afrique francophone entre autres.
            <br>
            Le premier système de télédiffusion est la norme NTSC qui vit la naissance de la télévision couleurs aux Etats-Unis en 1953. Ce système s'exporta en France
            sous une nouvelle norme SECAM et en Allemagne sous la norme PAL plus récente. Ces systèmes seront relayés bientôt par la télévision numérique et sont donc
            appelés à disparaître.
        </p>

        <h2 id="svp">Sega Virtua Processor</h2>
        <p>
            Le <em>Sega Virtua Processor</em> est un microprocesseur introduit par Sega en 1994 pour apporter à la MegaDrive des fonctionnalités 3D. C'est une puce
            spécialisée dans le calcul 3D intégrée directement à la cartouche de jeu. Le seul titre à l'avoir utilisée est Virtua Racing qui fut une véritable prouesse
            technique pour l'époque, marquant l'apogée technologique des consoles 16 bits. Mais bon, ça fait cher la puce pour un seul jeu... Le SVP est la réponse de
            Sega au SuperFX de Nintendo, lequel est utilisé dans certains jeux sur Super Nintendo.
        </p>

        <h2 id="signetics">Signetics 2650</h2>
        <p>
            Signetics est un fabricant de circuits intégrés des années 60-70. Son nom provient du terme anglais <em>SIGnal NETwork Integrated Circuits</em>. Il est à
            l'origine de nombreux circuits intégrés, circuits Dolby, circuits analogiques, clones de processeurs Motorola, dont les premières consoles de jeux Atari. Ses
            fondateurs sont également à l'origine de la société Fairchild Semiconductor. L'entreprise est rachetée en 1975 par Philips et fait maintenant partie de
            Philips Semiconductors. Une autre partie de l'entreprise est restée en Corée du Sud où elle a gardé son nom d'origine et fait toujours partie du Young Poong
            Group.
        </p>

        <h2 id="southbridge">Southbridge</h2>
        <p>
            Le southbridge est l'une des deux puces dans le noyau chipset de la carte mère, l'autre étant le Northbridge . Le southbridge implémente les capacités plus
            lentes de la carte mère dans un northbridge. Dans les systèmes avec Intel chipsets, le southbridge est nommé <em>I/O Controller Hub</em>, tandis que AMD nomme
            son southbridge <em>Fusion Controller Hub</em> depuis l'introduction de sa <em>Fusion AMD Accelerated Processing Unit</em>. Le southbridge peut se distinguer
            du northbridge de ne pas être directement connecté à la CPU . Au contraire, le northbridge lie le southbridge à la CPU. Grâce à l'utilisation de circuits de
            canal intégré au contrôleur, le Northbridge peut relier directement des signaux à partir des unités d'I/O à l'unité centrale de commande et d'accès aux
            données. Un chipset southbridge gère toutes les fonctions d'I/O d'un ordinateur, comme l'USB, l'audio, le BIOS du système, le bus ISA, le contrôleur
            d'interruption et les canaux IDE. Différentes combinaisons de puces Southbridge et Northbridge sont possibles, mais ces deux types de puces sont conçues pour
            fonctionner ensemble. Il n'y a pas de norme industrielle pour l'interopérabilité entre les différentes conceptions fondamentales du chipset logique.
        </p>

        <h2 id="superh">SuperH</h2>
        <p>
            La famille de processeurs SuperH est développée par Hitachi au début des années 90. Sega et Hitachi forment une coentreprise en 1993 pour développer un
            nouveau processeur pour la console de jeux Sega Saturn. Cette collaboration résulte en la création du SH-21. Deux processeurs SH-2 et un processeur SH-1 sont
            finalement utilisés dans la Saturn, et deux processeurs SH-2 dans la console Sega MegaDrive 32X. Quelques années plus tard, le SH-3 rejoint la famille. Il
            ajoute une <em>Unité de Gestion Mémoire</em>, une gestion de mémoire cache, et une extension DSP nommée SH-3-DSP. Il est entre autres utilisé par des
            périphériques tournant sous Windows CE.
            <br>
            Le SH-4 est le fruit d'un développement commun entre Hitachi et STMicroelectronics qui collaborent à partir de 1997 avant de former une coentreprise nommée
            SuperH en 2001. Il dispose d'une architecture superscalaire, d'une unité de calcul en virgule flottante vectorielle et d'un contrôleur PCI. Il est choisi par
            Sega pour sa console Dreamcast. Ces processeurs disposent d'un codage de taille fixe des instructions sur 16 bits, qui représente un avantage par rapport aux
            autres processeurs RISC utilisant des instructions sur 32 bits. Moins de mémoire vive et moins de cache sont nécessaires, ce qui se traduit en un coût de
            fabrication plus faible, ainsi qu'une dissipation thermique plus faible qui permet de se passer de ventilateur pour le processeur.
            <br>
            Hitachi et Mitsubishi Electric forment une coentreprise nommée Renesas Technology en 2003, qui reprend la participation de STMicroelectronics et devient ainsi
            le propriétaire de la licence des processeurs SH. Les derniers brevets liés au SH-2 sont arrivés à échéance en 2014. A la LinuxCon Japan, l'Open Processor
            Foundation annonce en 2015 son implémentation ouverte d'une architecture compatible SH-2, nommé J-2. Cette fondation implémente une architecture compatible
            SH-4 depuis l'échéance des brevets en 2016. Les raisons données pour l'implémentation de nouveaux processeurs sur cette architecture sont multiples : forte
            densité de code comparée aux autres architectures RISC 32-bit telles ARM ou MIPS, support existant de compilateurs et de systèmes d'exploitation, bas coût de
            fabrication.
        </p>

        <h2 id="s11">System 11</h2>
        <p>
            Après avoir essayé d'égaler les capacités de la console de jeu vidéo PlayStation de Sony avec son System 22 et Super System 22 - univers et graphisme en 3D -,
            Namco adopte du matériel déjà existant, la fameuse PlayStation. Ce fut un choix judicieux à l'époque ou ce hardware permet l'explosion de l'univers 3D dans
            les jeux vidéo. Namco commercialise en décembre 1994 le tout nouveau System 11 avec "Tekken" comme premier jeu.
            <br>
            Techniquement, comme le Sony ZN-1, le système se base sur le hardware de la PlayStation, ce qui explique que de nombreux jeux System 11 soient convertis sur
            cette console. Le System 11 profite de toutes les capacités de la toute nouvelle PlayStation à l'époque… Le processeur central nommé R3000A, est construit par
            LSI Logic sur une technologie Silicon Graphics - RISC - mais tourne à des fréquences supérieures à celle de la console. Il intègre le
            <em>Geometry Transformation Engine</em> - Moteur de transformation de géométrie - et le <em>Data Decompression Engine</em> - Moteur de décompression de
            données - qui permettent d'obtenir des capacités exceptionnelles en comparaison du matériel concurrent de l'époque. Le processeur graphique d'origine de la
            PlayStation est utilisée, mais le processeur sonore est remplacé par un processeur Namco C76 basé sur un Mitsubishi M37702 et une puce audio Namco C352. Ce
            système - ainsi que le System 12 et le System 10 - devient un système charnière pour Namco qui utilise dorénavant du matériel déjà développé. Le System 11
            accueille des jeux notables - "Soul Edge" - et l'emblématique jeu de la société Namco, "Tekken".
        </p>

        <h2 id="s32">System 32</h2>
        <p>
            Le System 32 est le premier système à apparaitre dans les salles d'arcade. Techniquement, il se constitue de parties de précédents systèmes d'arcade de Sega :
            système d'affichage de background - fond d'écran - du System 16, le système de son du System 18, le système de sprites du Y Board. Chaque partie est
            légèrement évoluée mais reste basiquement le même matériel. En effet, à cette époque Sega est en train de développer le Model 1 et décide de ne pas créer
            entièrement un nouveau système. Le premier jeu 32 bits de l'histoire de Sega est
            <a href="https://www.youtube.com/watch?v=1PE8aBvd0Z4" title="Rad Mobile">Rad Mobile</a> en 1991, qui sort sur trois bornes différentes : une borne classique,
            une version sitdown et une autre version dans la borne prototype R360.
            <br>
            Le System Multi 32 est une évolution du système permettant l'affichage de deux écrans et le jeu en simultané. Le matériel gérant les fonctions de background,
            de son et de sprite sont les mêmes, mais le processeur principal est le plus puissant NEC V70. Le son est légèrement remanié et le système embarque un YM3438,
            un chipset MultiPCM Sega de 28 canaux. Le System Multi 32 est un système de transition qui ne connait que 4 jeux, au même titre que le H1 System, matériel et
            concept similaire qui a également une durée de vie très courte puisqu'un seul jeu - "Cool Riders" - est commercialisé.
        </p>

        <h2 id="tflop">Téraflop</h2>
        <p>
            Le nombre de FLOPS représente le nombre d’opérations que peut effectuer un processeur chaque seconde sur des nombres à virgule. Ce genre de calcul est bien
            plus complexe que de simples opérations sur des nombres entiers. On calcule ce nombre de FLOPS à partir de nombres 32 bits - on parle alors de simple
            précision ou SP ou FP32 -, ou d’un nombre 64 bits - double précision ou DP ou FP64 -. La plupart du temps, on utilise un résultat simple précision sur 32 bits,
            donc FP32. Autrement dit, le nombre de FLOPS représente la puissance de calcul mathématique brute d’un processeur. La puce graphique, ou GPU, d’une carte
            graphique, n’est rien d’autre qu’un processeur spécialisé, on peut tout aussi bien calculer le nombre de FLOPS d’un CPU ou d’un GPU ou tout autre type de
            processeurs. Le préfixe <em>téra</em> est un système international d’unités pour représenter 1 milliard, ou 1 000 000 000 000. On connait ce préfixe
            aujourd’hui pour les espaces de stockage, notamment sur les disques durs, de 1 ou plusieurs téraoctets - To - et on voit également de nouvelles puissances
            exprimées en <em>pétaflops</em>.
            <br>
            Concrètement, essayons de "visualiser" ce principe de téraflop sur les consoles :
        </p>
            <table>
                <caption>Comparatif de puissance des consoles</caption>
                <thead>
                    <tr>
                        <th>Marque</th>
                        <th>Console</th>
                        <th>Puissance</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Microsoft</td>
                        <td>XBox 360</td>
                        <td>0,240 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>XBox One</td>
                        <td>1,31 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>XBox One X</td>
                        <td>6 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>XBox Series X</td>
                        <td>12 TFlops</td>
                    </tr>
                    <tr>
                        <td>Nintendo</td>
                        <td>Wii</td>
                        <td>0,0012 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Wii U</td>
                        <td>0,35 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Switch</td>
                        <td>0,5 TFlops</td>
                    </tr>
                    <tr>
                        <td>Sony</td>
                        <td>PlayStation 3</td>
                        <td>0,4 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>PlayStation 4</td>
                        <td>1,84 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>PlayStation 4 pro</td>
                        <td>4,2 TFlops</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>PlayStation 5</td>
                        <td>10,28 TFlops</td>
                    </tr>
                    <tr>
                        <td>Google</td>
                        <td>Stadia</td>
                        <td>10,7 TFlops</td>
                    </tr>
                </tbody>
            </table>

        <h2 id="usb">USB</h2>
        <p>
            Le terme anglais <em>Universal Serial Bus</em> ou USB est une norme relative à un bus informatique en série qui sert à connecter des périphériques
            informatiques à un ordinateur ou à tout type d'appareil prévu à cet effet - tablette, smartphone... - Le bus USB permet de connecter des périphériques à chaud
            - quand l'ordinateur est en marche - et en bénéficiant du Plug and Play qui reconnaît automatiquement le périphérique. Il alimente les périphériques peu
            gourmands en énergie - clé USB, disques SSD et avec les dernières version des normes - Prise USB type C - des appareils réclamant plus de puissance - 60W en
            standard, 100W max - -. La version 1.0 de l'USB est apparue en janvier 96, puis ce connecteur s'est généralisé dans les années 2000 pour connecter souris,
            clavier d'ordinateur, imprimantes, clés USB et autres périphériques sur les ordinateurs personnels. Les performances de l'USB en débits se sont grandement
            améliorées au fil des versions : de 1,5 Mbit/s pour la version 1.0 à 20 Gbit/s théoriques pour la version 3.2.
        </p>

        <h2 id="v2000">V2000</h2>
        <p>
            Le Video 2000 est un format de cassette à ruban vidéo. Produit de Philips et Grundig destiné aux enregistrements télévisuels. Ce format a vécu de 1979 à 1988,
            exclusivement en Europe avant de céder le pas face à ses concurrents VHS et Betamax. Le V2000 apporte de nombreuses innovations, comme enregistrer sur les
            deux faces de la cassette et le <em>Dynamic Track Following</em> - suivi dynamique de piste -, technologie dépassant alors tous les autres formats vidéo,
            grand public comme professionnels. Malgré ces avancées, le V2000 est un échec commercial car dès 1985, le marché européen de l'enregistrement et de la lecture
            vidéo est très majoritairement conquis par les appareils au format VHS.
        </p>

        <h2 id="vhs">Video Home System - VHS</h2>
        <p>
            Le <em>Video Home System</em> désigne une norme d’enregistrement de signaux vidéo sur bande magnétique mise au point par la marque japonaise JVC à la fin des
            années 70 et disparut progressivement au cours des années 2000. Le sigle VHS signifie initialement <em>Vertical Helical Scan</em> - balayage hélicoïdal
            vertical - mais cette désignation est abandonnée rapidement car trop difficile à mémoriser pour le grand public. Le format VHS arrive peu de temps après son
            concurrent direct, le Betamax créé par l'entreprise japonaise Sony. Un troisième concurrent tente de rivaliser avec les deux premiers sans succès, le V2000 du
            néerlandais Philips.
            <br>
            Son utilisation se borne à la large diffusion et à l’enregistrement des émissions de télévision pour les particuliers. Elle est utilisée dans de nombreux
            caméscopes que ce soit avec la cassette VHS ou avec sa petite sœur la VHS-C mais sur ce marché, le 8 mm puis le Hi8 prendront l'ascendant. La VHS s’impose
            comme la norme de la vidéo grand public face à ses concurrents, le Betamax de Sony et le V2000 de Philips.
            <br>
            La VHS offre une qualité d’image en général inférieure à celles des supports numériques. Les premières générations d'appareils ont un son mono avec un faible
            rapport signal/bruit, une bande passante restreinte et un taux de distorsion élevé. L’ajout d'une piste Hi-Fi stéréo permet de bénéficier d’un son d'une
            qualité largement supérieure. Ce format subsiste encore plus de 30 ans après sa création car il a cinq avantages : son universalité, sa qualité, sa fiabilité,
            sa solidité, son prix.
        </p>

        <h2 id="hdr">Vidéo HDR</h2>
        <p>
            La vidéo HDR ou <em>Vidéo à Haute Gamme Dynamique</em> est un standard d'image numérique permettant de représenter des niveaux de luminosités plus élevés que
            la limite de 100cd/m² des vidéos standards - SDR -. Selon le format utilisé, une vidéo HDR permet de représenter des pics de luminosité jusqu'à une valeur de
            1 000 nits via le format HLG ou jusqu'à 10 000 nits via les formats Dolby Vision2, HDR10, HDR10+. Ainsi, le HDR permet d'augmenter la gamme dynamique pouvant
            être enregistrée dans une image ou une vidéo numérique. Le HDR permet d'obtenir des hautes-lumières plus lumineuses, plus détaillées et plus saturées en
            couleur. Les technologies liées permettent également d'améliorer le détail dans les zones sombres d'une image numérique.
            <br>
            Certaines technologies permettent d'augmenter la quantité de pixels d'une image numérique. Les standards HD, Full HD, Ultra HD, 4K, 8K... permettent
            d'augmenter la définition d'une image. Le nombre d'images par seconde - 24, 25, 30, 60, 120... - permet d'augmenter la quantité de pixel par seconde. D'autres
            technologies permettent d'augmenter la qualité des pixels. C'est le cas pour le HDR. Les normes de luminosité des vidéos standards sont basées sur les
            caractéristiques des télévisions à tube cathodique cependant la luminosité maximale des écrans modernes est en constante augmentation et dépasse ces limites.
            Les standards d'images HDR sont développés dans le but de s'affranchir de ces limites. La dynamique des caméras et appareils photographiques dépasse aussi
            largement celle des vidéos SDR notamment à l'aide de la technologie de capture d'image HDR. Les images générées par ordinateur - comme les effets spéciaux
            numériques ou les images des jeux vidéo - ont un rendu interne effectué avec une grande dynamique pouvant contenir du HDR. Les études montrent qu’augmenter la
            luminosité et le détail des hautes-lumières ainsi que le détail dans les zones sombres permet d’améliorer l’expérience visuelle du spectateur.
        </p>

        <h2 id="vrc">Videogame Rating Council</h2>
        <p>
            Le <em>Videogame Rating Council</em> - VRC - est engendré par Sega of America en 1993 pour évaluer tous les jeux vidéo mis en vente aux États-Unis et au
            Canada sur Sega Master System, Genesis, Game Gear, Sega CD et Pico. La note doit être clairement affichée sur le devant de la boîte, mais leur apparition dans
            les publicités pour le jeu vidéo est facultative. Il est ensuite remplacé par le <em>Conseil d'Evaluation des Logiciels de Divertissement</em> à l'échelle de
            l'industrie.
            <br>
            Avec le début de l'ère 16 bits à la fin des années 80, le contenu des jeux devient plus réaliste. La fidélité graphique et audio accrue des produits rend les
            scènes violentes plus explicites, en particulier celles contenant du sang. Alors que la controverse découle du réalisme de cette violence, les jeux "Mortal
            Kombat" et "Night Trap" entrent sous les projecteurs. "Mortal Kombat" est un jeu de combat brutal et "Night Trap" est un FMV sur Sega CD où les joueurs
            protègent une soirée pyjama contre les vampires. Les jeux sont au centre des audiences fédérales tenues du 9 décembre 1993 au 4 mars 1994 par les sénateurs
            américains Joseph Lieberman et Herbert Kohl. Lieberman explique ce qu'il a ressenti à propos des jeux vidéo lors de l’un de ces procès :
            <cite>
                &#10077;Au lieu d’enrichir l’esprit d’un enfant ... ces jeux apprennent à un enfant à aimer infliger la torture&#10080;
            </cite>
            , rien que ça ! Bon alors, je vous préviens: Biden, c'est le même, au vu de certaines de ses déclarations envers les développeurs ! En conséquence,
            l'industrie du jeu vidéo a un an pour créer son propre système de classification ou pour s'en voir imposer un par le gouvernement fédéral.
            <br>
            Avant les auditions, Sega prend conscience des rumeurs politiciennes selon lesquelles le contenu des jeux vidéo est surveillé. Alors que Sega se prépare à
            sortir le controversé "Mortal Kombat" pour Sega Genesis, la société travaille à créer son propre système de notation afin de pouvoir commercialiser le jeu
            comme un jeu mature non destiné aux enfants. Sega essaie d'abord de délivrer une licence au système de classification de la
            <em>Motion Picture Association of America</em> - MPAA -, mais la MPAA refuse. Grand bien leur fasse, Sega crée son propre Videogame Rating Council et
            révèle son existence le 24 mai 1993. Le conseil est composé d'experts en éducation, en psychologie et en sociologie nommés par Sega. Le VRC est l'un des
            nombreux groupes de notation à apparaître - parmi eux, le système de notation 3DO -. Le VRC classe les jeux Sega en trois catégories basées sur l'âge : GA
            - "Tout public" -, MA-13 - "Public mature" - et MA-17 - "Pour les adultes" -. Bien évidemment, les journalistes et les associations de consommateurs
            critiquent les imprécisions et les incohérences délivrées et d'autres entreprises ne veulent pas que Sega soit responsable de l'organisation de notation. De
            mon avis perso, Sega crée le VRC à ce moment-là car il sait pertinemment que "Mortal Kombat" risque de se retrouver interdit à la vente surtout aux US, friand
            de procès en tout genre ! L'éditorial du numéro de janvier 94 d'Electronic Gaming Monthly critique aussi Sega pour ne pas avoir informé et éduqué le public
            sur ce système de notation, notant en particulier que les notes apparaissent sur les boîtes de jeu mais pas dans les publicités, et que la plupart des
            parents n'ont pas la moindre idée de ce que signifient les notes de Sega, ne savent pas qu'elles existent, soit ne savent pas pourquoi le jeu obtient telle
            note. À la suite des auditions, l'industrie du jeu crée l'<em>Interactive Digital Software Association</em> en avril 94 proposant un système de notation
            universel. La proposition est adoptée par le Congrès des États-Unis en juillet et le <em>Entertainment Software Rating Board</em> - ESRB - est fondé en
            septembre. Le VRC se retrouve ainsi remplacé par l'ESRB.
        </p>

        <h2 id="wifi">Wi-Fi</h2>
        <p>
            Le Wi-Fi est un ensemble de protocoles de communication sans fil. Un réseau Wi-Fi permet de relier par ondes radio plusieurs appareils informatiques -
            ordinateur, routeur, smartphone, modem Internet... - au sein d'un réseau informatique afin de permettre la transmission de données entre eux. Apparues pour la
            première fois en 1997, les normes IEEE 802.11 - ISO/CEI 8802-11 -, qui sont utilisées internationalement, décrivent les caractéristiques d’un réseau local
            sans fil - WLAN -. La marque déposée <em>Wi-Fi</em> correspond initialement au nom donné à la certification délivrée par la Wi-Fi Alliance - Wireless Ethernet
            Compatibility Alliance -, organisme ayant pour mission de spécifier l’interopérabilité entre les matériels conformes à la norme 802.11 et de vendre le label
            "Wi-Fi" aux matériels répondant à ses spécifications. Pour des raisons de facilité d’usage et de marketing, le nom de la norme se confond aujourd’hui avec le
            nom de la certification. Ainsi, un réseau Wi-Fi est en réalité un réseau répondant à une des normes IEEE 802.11.
            <br>
            Grâce aux normes Wi-Fi, il est possible de créer des réseaux locaux sans fil à haut débit. En pratique, le Wi-Fi permet de relier des ordinateurs portables,
            des machines de bureau, des PDA ou même des périphériques à une liaison haut débit : de 11 Mbit/s théoriques ou 6 Mbit/s réels pour la norme 802.11b, à 54
            Mbit/s théoriques ou environ 25 Mbit/s réels pour la 802.11a ou 802.11g, 600 Mbit/s théoriques pour la 802.11n et 1,3 Gbit/s théoriques pour la 802.11ac
            normalisé depuis décembre 2013. La portée peut atteindre plusieurs dizaines de mètres en intérieur - généralement entre une vingtaine et une cinquantaine de
            mètres - s'il n'y a aucun obstacle gênant - mur en béton... - entre l’émetteur et l’utilisateur. Les fournisseurs d’accès à Internet peuvent établir un réseau
            Wi-Fi connecté à Internet dans une zone à forte concentration d’utilisateurs - gare, aéroport, hôtel, train... -. Ces zones ou points d’accès sont appelés
            bornes, points d’accès Wi-Fi ou <em>hot spots</em>.
            <br>
            Le terme Wi-Fi suggère la contraction de <em>Wireless Fidelity</em>, par analogie au terme Hi-Fi pour High Fidelity. Cependant, bien que la Wi-Fi Alliance ait
            elle-même employé fréquemment ce terme dans divers articles de presse internet, selon Phil Belanger, membre fondateur de la Wi-Fi Alliance, Wi-Fi n'a jamais
            eu de réelle signification. Il s'agit bien néanmoins d'un jeu de mots avec Hi-Fi. Il est inventé par la société Interbrand, spécialisée dans la communication
            de marque, afin de proposer un terme plus attractif que la dénomination technique IEEE 802.11b Direct Sequence. Interbrand est également à l'origine du logo
            rappelant le symbole du Yīn et du Yang. En français, le terme est plus souvent utilisé au masculin qu'au féminin. Toutefois, si vous séjournez en Angleterre,
            dîtes "Wouaille Faille" car "Wi Fi" signifie que "nous puons"... et vous savez combien un anglais est susceptible surtout s'il a un français en face de lui...
        </p>

        <h2 id="xross">XrossMediaBar</h2>
        <p>
            Le XrossMediaBar - dire Cross-Media Bar - est une interface utilisateur graphique - GUI en anglais - développée par Sony Computer Entertainment. L'interface
            comporte des icônes réparties horizontalement sur l'écran. La navigation déplace les icônes au lieu d'un curseur. Ces icônes sont utilisées comme catégories
            pour organiser les options disponibles pour l'utilisateur. Lorsqu'une icône est sélectionnée sur la barre horizontale, plusieurs autres apparaissent
            verticalement, au-dessus et en dessous. A leur tour, ils peuvent être sélectionnés par les directions haut et bas sur un pavé directionnel. Utilisé à
            l'origine sur la PSX - une PlayStation 2 avec un enregistreur vidéo numérique intégré uniquement sorti au Japon -, le XMB est utilisé comme interface par
            défaut sur la PlayStation Portable et la PlayStation 3. Il est également utilisé dans les téléviseurs WEGA haut de gamme, le BRAVIA , le téléviseur OLED
            Sony, les décodeurs HDTV, les lecteurs Blu-ray et certains appareils photo Sony Cyber-shot depuis 2006. L'interface remporte le
            <em>Technology & Engineering Emmy Award</em>  pour "Innovation et réalisation exceptionnelles dans la technologie des médias avancés pour la meilleure
            utilisation de la technologie d'affichage et de présentation des médias personnels" en 2006. Le XMB est progressivement abandonné depuis la PlayStation Vita,
            qui adopte une nouvelle interface tactile appelée LiveArea.
        </p>
    </article>
</main>

<footer>
    <p>
        &copy; 2021 - Lyasen - tous droits réservés
    </p>
    <ul>
        Mes sites recommandés :
        <li>ActuGaming.net</li>
        <li>GameBlog</li>
        <li>Gamekult</li>
        <li>JeuxActu</li>
        <li>jeuxvideo.com</li>
        <li>YouTube</li>
    </ul>
    <p>
        <a href="mailto:consolStory@protonmail.com">Nous contacter</a>
    </p>
</footer>
</body>

</html>